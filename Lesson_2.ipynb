{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - Feedforward models\n",
    "\n",
    "This lesson will present the *dataflow* paradigm and its application to Artificial Neural Networks.\n",
    "The students will implement from scratch all the operations required to create their first Deep model, and finally use it to solve a regression task.\n",
    "\n",
    "### Summary\n",
    "\n",
    "* [Classical visualization of ANNs](#classicalANNs)\n",
    "* [The *dataflow* paradigm](#dataflow)\n",
    "* [Operations](#operations)\n",
    "* [Running a *dataflow* program](#rundataflowprog)\n",
    "* [Reinterpreting the *backpropagation* algorithm](#dataflowbackprop)\n",
    "* [Batch processing](#batch)\n",
    "* [The *Boston housing prices* problem](#bostonhousing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=classicalANNs></a>\n",
    "### Classical visualization of ANNs\n",
    "\n",
    "Our way to think to problems is strongly influenced by the way we visualize them.\n",
    "Artificial Neural Networks visualization has been strongly influenced by biological neural networks.\n",
    "The fundamental processing units, the neurons, are usually depicted as small circles plus their connections.\n",
    "In the picture below, we highlighted a neuron nucleus in gold, its synapsis (i.e. incoming signals channels) in green and its dendrites (i.e. outgoing signals channels) in turquoise.\n",
    "\n",
    "<img src='figures/neuralnet_classic.png', width=360, height=360></img>\n",
    "\n",
    "Although simple, this representation system is highly misleading from a mathematical modelling perspective.\n",
    "In fact, it induces a graph representation in our mind, thinking to neurons' nucleuses as nodes and connections as directed edges.\n",
    "It perfectly depicts the structure of the ensemble of fundamental units, but does not convey much of the **hierarchy of feature maps** that each layer of neurons applies to the preceding layers' features.\n",
    "\n",
    "We can think to the ensemble of the states of the neurons in a particular layer as coordinates of points in a vector space (or its natural extension, a **tensor space**).\n",
    "\n",
    "<img src='figures/neuralnet_classic_hierarchy.png', width=360, height=360></img>\n",
    "\n",
    "First, the ANN *looks* at a point $x \\in X$ using its input layer (the cyan one in figure); the connections between the input layer and the second (hidden) layer then transform this point into a point $\\phi(x) \\in \\Phi$ in what is called a *latent representation* or **feature** (the green one in figure); finally, the connections from the hidden layer to the output layer trasform this feature into a point $y = f(\\phi(x))$ (the magenta one in figure) in the desired output space $Y$.\n",
    "\n",
    "The main change of paradigm in visualization of ANNs is passing from node=neuron/edge=connection graphs to **node=feature map/edge=tensor data structure** graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=dataflow></a>\n",
    "### The *dataflow* paradigm\n",
    "\n",
    "[Dataflow](https://en.wikipedia.org/wiki/Dataflow_programming) is a programming model that thinks to programs as computational graphs, graphs which **nodes are operations** and which **edges contain data that are consumed and produced by the operations** they connect.\n",
    "We will call consumed data *operands* and produced data *results*.\n",
    "\n",
    "Why is the *dataflow* paradigm so important for ANNs research?\n",
    "The main reason is that graph representation of a program exposes important computational optimizations:\n",
    "* **concurrency**, due to the explicit definition of dependencies (edges);\n",
    "* **distributed computing**, thanks to the operations placement which is possible due to the operations modeling (nodes).\n",
    "\n",
    "The degree of concurrency enabled by parallel architectures and parallel programming models played a critical role in the Artificial Neural Networks boom of the last ten years.\n",
    "\n",
    "<img src='figures/google_dl.png', width=480, height=480></img>\n",
    "\n",
    "Mastering the **dataflow programming model** requires to approach the programming problem in four stages.\n",
    "The first three stages compose a **building phase** during which the graph is assembled.\n",
    "The last stage represents the **execution phase**, when the graph is *brought to life* flowing data through it.\n",
    "\n",
    "The four stages are:\n",
    "* **design** an analytical model for you problem;\n",
    "* **identify the transformations** required by the model;\n",
    "* **assemble these tranformations consistently**, i.e. respecting their hierarchical dependencies;\n",
    "* **flow data through the model**, feeding suitable input data to the first transformations and then executing other operations as soon as their required operands become available.\n",
    "\n",
    "To get a grasp on these concepts, we will go step by step through an example.\n",
    "\n",
    "During a data analysis, we **designed a statistical model**, parametric in $\\theta=(b_1, w_1, w_2)$, expressed by the analytical function:\n",
    "\n",
    "$$\\begin{align} f(x, \\theta=(b_1, w_1, w_2)) &= h(x, b_1, w_1)w_2 \\\\ &= \\sigma(\\nu(x)w_1 + b_1)w_2 \\\\ &= \\sigma((x - \\bar{x})w_1 + b_1)w_2 \\end{align}$$\n",
    "\n",
    "This decomposition highlights the dependencies of each stage of computation on preceeding elements.\n",
    "Thus we can proceed to **identify the required elementary transformations** proceeding backward from the last (higher level) operation, iteratively asking **\"Which operands are required to compute this feature?\"**:\n",
    "* the last operation is $f(x, \\theta=(b_1, w_1, w_2)) = h(x, b_1, w_1)w_2$, that it is the product between a vector $h$ and a matrix $w_2$; these operands come from distinct operations;\n",
    "* $w_2$ is a parameter of our model, so it is data that should just be emitted by some *emitting operation* which has no required dependencies; since parameters can be changed, this operation should somehow allow to emit different data depending on the moment it is executed;\n",
    "* $h = h(x, b_1, w_1)$ is itself the otput of an operation $h(x, b_1, w_1) = \\sigma(\\nu(x)w_1 + b_1)$, which requires as operand the quantity $s_1 = \\nu(x)w_1 + b_1$;\n",
    "* $s_1$ is the sum of two operands, namely $\\nu(x)w_1$ and $b_1$;\n",
    "* we can apply to $b_1$ the same reasoning that we applied to $w_2$;\n",
    "* $\\nu(x)w_1$ is the vector-matrix product of the two  operands $\\nu(x)$ and $w_1$;\n",
    "* we can apply to $w_1$ the same reasoning that we applied to $w_2$ and $b_1$;\n",
    "* $\\nu(x) = x - \\bar{x}$ is obtained as the difference betweend the independent operands $x$ and $\\bar(x)$;\n",
    "* $\\bar{x}$ is not a parameter, but an external constant that should be always emitted equal to itself (where a parameter could change);\n",
    "* finally, $x$ is the actual input to the statistical model, the observation of the reality: it should be emitted by an operation which is be fed by the real world.\n",
    "\n",
    "This analysis corresponds to the computational graph depicted below.\n",
    "\n",
    "<img src='figures/dataflow.png', width=480, height=480></img>\n",
    "\n",
    "We see that we have **input nodes** that contain:\n",
    "* data that can be fed in by the real world: **placeholder** operation;\n",
    "* constant information: **constant** operation;\n",
    "* parametric information, that can be changed as required: **variable** operations.\n",
    "\n",
    "Then, we have *real* operations in the form of **internal nodes**:\n",
    "* **sum** node;\n",
    "* **vector-matrix multiplication** nodes;\n",
    "* **sigmoid activation** node.\n",
    "\n",
    "We said before that the intermediate feature spaces represented by ANNs layers actually are vector or tensor spaces.\n",
    "Due to the inherent structure of vector and tensor spaces, points in these spaces are naturally encoded by multidimensional arrays data structures.\n",
    "Thus, **operations** (the circles representing the graph nodes) eventually consume and produce data in the form of **tensors** (the squares on the edges).\n",
    "Usually, the output tensor of the last feature map is not considered as an edge, since it is not consumed by any operation.\n",
    "\n",
    "To simplify the program management, operation placement and data transfer, operations and their output tensor are usually merged into a single implemented object called `Node`.\n",
    "\n",
    "<img src='figures/dataflow_implementation.png', width=480, height=480></img>\n",
    "\n",
    "Such an object should store basically three attributes:\n",
    "* `inbound_nodes`, a list of the **nodes whose output is required by the current node**, i.e. the nodes that provide operands to it;\n",
    "* `outbound_nodes`, a list of **nodes that will use the current node's output**, i.e. the nodes to which it provides operands;\n",
    "* `state`, an array to store **the result of the operation**; the output tensor itself!\n",
    "\n",
    "The node should also have a method:\n",
    "* `forward`, an algorithm to consume the inputs (operands) and produce its output (result).\n",
    "\n",
    "When a `Node` object is created, it should be linked to the `Node`s that contain its required operand.\n",
    "Thus, the `__init__` method of such a class should update the information about the graph's connectivity, taking a `list` of inbound `Node`s and communicating to them that the currently created `Node` requires their state (i.e. updating their `outbound_nodes` attribute).\n",
    "Notice that the connectivity information added by the constructor method of a `Node` is local, since it regards only the edged which have the current `Node` as an extremum.\n",
    "\n",
    "To visualize this, the next figure highlights a subgraph realizing the feature map $s_1 = l_1(x) = \\nu(x)w_1 + b_1$; this subgraph is composed by:\n",
    "* an arbitrary node as the current node (magenta);\n",
    "* its `inbound_nodes` (orange), containing the operands $\\nu(x)$, $w_1$ and $b_1$;\n",
    "* its `outbound_nodes` (sea green), where the output tensor $s_1$ is directed.\n",
    "\n",
    "<img src='figures/dataflow_implementation_Node.png', width=480, height=480></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, inbound_nodes=list()):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        for node in self.inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "        self.outbound_nodes = list()\n",
    "        self.state = None\n",
    "    \n",
    "    def forward(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=operations></a>\n",
    "### Operations\n",
    "\n",
    "#### Input operations\n",
    "The basic operations of a *dataflow* graph are input operations.\n",
    "These operations provide *raw data* to the program, data that is not preprocessed or transformed before being fed to the graph.\n",
    "\n",
    "This information comes in three form:\n",
    "* `Constant`, an operation which emits an operand always equal to itself; the value of the operand should be communicated to the operation at its creation;\n",
    "* `Placeholder`, an operation which emits an operand that has been read from the external environment; its `forward` method should thus use a parameter `value` which reads the current observation from the environment;\n",
    "* `Variable`, an operation which emits an operand that must be initialized by the operation's constructor method; this value is stored as the operation's state; the difference from `Constant` operations will be described in the paragraph about [backpropagation](#dataflowbackprop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Constant(Node):\n",
    "    def __init__(self, value):\n",
    "        Node.__init__(self)\n",
    "        self.state = value\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class Placeholder(Node):\n",
    "    def __init__(self):\n",
    "        Node.__init__(self)\n",
    "        \n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.state = value\n",
    "            \n",
    "            \n",
    "class Variable(Node):\n",
    "    def __init__(self, initial_value):\n",
    "        Node.__init__(self)\n",
    "        self.state = initial_value\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear maps and affine transformations\n",
    "**Linear maps** are the most important transformations that can be applied to vector and tensor spaces.\n",
    "Let $X$ and $S$ be two vector spaces, that we will call the *input space* and the *score space* respectively.\n",
    "When both these spaces are finite-dimensional, with dimensions $n$ and $m$ respectively, a linear map\n",
    "\n",
    "$$l: X \\to S$$\n",
    "\n",
    "can be algebraically described by a **vector-matrix multiplication**\n",
    "\n",
    "$$\\begin{gather} xW = \\begin{pmatrix} x_1 & x_2 & \\dots & x_n \\end{pmatrix}\n",
    "                      \\begin{pmatrix} w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "                                      w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "                                      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                                      w_{n1} & w_{n2} & \\dots & w_{nm} \\end{pmatrix}\n",
    "\\end{gather}$$\n",
    "\n",
    "The result of this operation is a vector $s \\in S$ which components are called *scores*\n",
    "\n",
    "$$s_k = \\sum_{i=1}^{n}x_i w_{ik}, k = 1, \\dots m$$\n",
    "\n",
    "The geometric meaning of this transformation is clear: the $k$-th component of $s$ is the result of a projection operation of $x$ onto the one-dimensional subspace of $X$ idetified by the $k$-th column of $W$.\n",
    "\n",
    "$$s_k = \\langle x, W^{(k)} \\rangle, k = 1, \\dots m$$\n",
    "\n",
    "Linear maps are constrained to rotations and homothecies around the origin ([polar decomposition](https://en.wikipedia.org/wiki/Polar_decomposition)), and cannot account for translations.\n",
    "To model such dependencies, a translation is usually added under the form of a **bias vector** $b \\in S$:\n",
    "\n",
    "$$\\begin{align} l: \\, &X \\to S \\\\ &x \\mapsto xW + b \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, x, w):\n",
    "        Node.__init__(self, inbound_nodes=[x, w])\n",
    "        \n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        w = self.inbound_nodes[1].state\n",
    "        self.state = np.dot(x, w)\n",
    "        \n",
    "        \n",
    "class Add(Node):\n",
    "    def __init__(self, s, b):\n",
    "        Node.__init__(self, inbound_nodes=[s, b])\n",
    "        \n",
    "    def forward(self):\n",
    "        s = self.inbound_nodes[0].state\n",
    "        b = self.inbound_nodes[1].state\n",
    "        self.state = s + b\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear activations\n",
    "As stated by the Universal Approximation Theorem, the power of ANNs emerges when they are able to **distort space to extract non-linear features**.\n",
    "\n",
    "In order to achieve this, the output scores of linear transformations $s_k, k=1, \\dots m$ should be filtered by **sigmoid** activations\n",
    "\n",
    "$$\\begin{align} y_k &= \\sigma(s_k) \\\\ &= \\frac{e^{s_k}}{e^{s_k} + 1}\\end{align}$$\n",
    "\n",
    "In practice, many non-linear functions can play the same role.\n",
    "For example, the **hyperbolic tangent**\n",
    "\n",
    "$$\\begin{align} y_k &= \\tanh(s_k) \\\\ &= 2\\sigma(s_k) - 1 \\end{align}$$\n",
    "\n",
    "or the Leaky Rectified Linear Unit (**LeakyReLU**)\n",
    "\n",
    "$$LeakyReLU_{q}(s_k) = \\begin{cases} qs_k, & \\mbox{if } s_k \\leq 0 \\\\ s_k, & \\mbox{if } s_k > 0 \\end{cases}$$\n",
    "\n",
    "where $q \\geq 0$ is a design constant (the case $q = 0$ yields the widespreadly used **ReLU** function).\n",
    "\n",
    "Why do they exist so many activation functions?\n",
    "Should not sigmoid suffice due to the UAT?\n",
    "Although in theory the LeakyReLU and the ReLU functions do not satisfy the hypothesis of the UAT, in real instances they do, and due to their minimal computational complexity with regard to sigmoids and hyperbolic tangents are thus largely used in real models.\n",
    "Nevertheless, sigmoid and hyperbolic tangents are used since they allow more straightforward statistical interpretations:\n",
    "* since $\\sigma(s_k) \\in (0, 1)$, sigmoid units can smoothly approximate boolean behaviour (e.g. the probability that a certain feature has been detected or not);\n",
    "* since $\\tanh(s_k) \\in (-1, 1)$ carries sign information, hyperbolic tangent units can test wether a certain feature is inhibitory or excitatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "        \n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0]\n",
    "        e_x = np.exp(x)\n",
    "        self.state = e_x / (1+e_x)\n",
    "        \n",
    "\n",
    "class Tanh(Node):\n",
    "    def __init__(self, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "        \n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0]\n",
    "        e_2x = np.exp(2*x)\n",
    "        self.state = (e_2x - 1.0) / (e_2x + 1.0)\n",
    "        \n",
    "        \n",
    "class LeakyReLU(Node):\n",
    "    def __init__(self, q, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "        self.q = q\n",
    "        \n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        mask = x > 0\n",
    "        mask = mask + q * (1-mask)\n",
    "        self.state = x * mask\n",
    "        \n",
    "        \n",
    "class ReLU(LeakyReLU):\n",
    "    def __init__(self, x):\n",
    "        LeakyReLU.__init__(self, 0, x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=rundataflowprog></a>\n",
    "### Running a *dataflow* program\n",
    "\n",
    "Up to now, we have developed all the components required to create a computational graph.\n",
    "But before we can run it, we have to accomplish the third stage, **assemble these operations consistently**.\n",
    "We can tackle this stage dividing it into two tasks:\n",
    "* assemble a graph;\n",
    "* automatically detect the computational dependencies between the operations in the graph.\n",
    "\n",
    "As for the assembly, opearations should be chained manually by the programmer.\n",
    "Since he is also the creator of the model, he knows which are the higher level features and which operands (i.e. which lower level abstractions) they require.\n",
    "For example consider a linear unit without non-linear activation as shown in the previous lesson.\n",
    "\n",
    "<img src='figures/linear_unit_revisited.png', width=480, height=480></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearUnitGraph():\n",
    "    def __init__(self):\n",
    "        self._build()\n",
    "        \n",
    "    def _build(self):\n",
    "        self.X = Placeholder()\n",
    "        self.W = Variable(np.random.randn(3, 1))\n",
    "        self.B = Variable(np.zeros((1, 1)))\n",
    "        self.s = Add(Linear(self.X, self.W), self.B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an assembled graph.\n",
    "But the `LinearUnitGraph` class still does not contain information on the order of execution of the operations.\n",
    "Consequently, it can not process automatically the calls to the `forward` methods of each node, because it might call them in an inconsistent order!\n",
    "\n",
    "To avoid calling `forward` methods by hand (which is also error prone), we can develop an algorithm to solve this problem.\n",
    "\n",
    "Remember that the constructor methods of the `Node`s take care of storing **local connectivity information**.\n",
    "Since all `Node` objects in a graph store local connectivity information, we can use it to reconstruct the **global topology** of the graph and to extract dependencies.\n",
    "Suppose the computational graph is composed by nodes $O_1, O_2, \\dots O_p$.\n",
    "We would like a permutation $O_{i_1}, O_{i_2}, \\dots O_{i_p}$ such that all `inbound_nodes` of node $O_{i_{\\bar{j}}}$ have indices $i_j$ such that $j < \\bar{j}$.\n",
    "This problem is known as **topological sorting**.\n",
    "\n",
    "There are many ways to compute the topological sorting of a graph given its connectivity information.\n",
    "We use [Kahn's algorithm](https://en.wikipedia.org/wiki/Topological_sorting#Algorithms) for our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_connectivity(input_nodes):\n",
    "    \"\"\"Create a description of the connections of each node in the graph.\n",
    "\n",
    "    Recurrent connections (i.e. connections of a node with itself) are excluded.\n",
    "\n",
    "    Args:\n",
    "        input_nodes (:obj:`list` of :obj:`Node`): the input operations of\n",
    "            the model.\n",
    "\n",
    "    Returns:\n",
    "        graph (:obj:`dict` of :obj:`dict` of :obj:`set`): a description of the\n",
    "            graph's connectivity in terms of inbound-outbound nodes of each\n",
    "            node.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    graph = dict()\n",
    "    nodes = input_nodes.copy()\n",
    "    while len(nodes) != 0:\n",
    "        # select a node\n",
    "        current_node = nodes.pop(0)\n",
    "        # if no information has been collected yet, set up dict entry\n",
    "        if current_node not in graph:\n",
    "            graph[current_node] = {'inbound': set(), 'outbound': set()}\n",
    "        # scroll through current node's outbound nodes\n",
    "        for node in current_node.outbound_nodes:\n",
    "            # skip recurrent connections (for RNN cells)\n",
    "            if node == current_node:\n",
    "                continue\n",
    "            # if no information has been collected yet, set up dict entry\n",
    "            if node not in graph:\n",
    "                nodes.append(node)\n",
    "                graph[node] = {'inbound': set(), 'outbound': set()}\n",
    "            # add reciprocal connectivity information\n",
    "            graph[current_node]['outbound'].add(node)\n",
    "            graph[node]['inbound'].add(current_node)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def topological_sort(input_nodes, graph):\n",
    "    \"\"\"Get a consistent sequence of operations on the given graph.\n",
    "\n",
    "    Args:\n",
    "        input_nodes (:obj:`list` of :obj:`Node`): the input operations of\n",
    "            the model.\n",
    "        graph (:obj:`dict` of :obj:`dict` of :obj:`set`): a description of the\n",
    "            graph's connectivity.\n",
    "\n",
    "    Returns:\n",
    "        sorted_nodes (:obj:`list` of :obj:`Node`): a sequence of operations\n",
    "            that ensures computational consistency of the model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sorted_nodes = list()\n",
    "    unlocked_nodes = input_nodes.copy()\n",
    "    while len(unlocked_nodes) != 0:\n",
    "        # select an inbound-free node and add it the sorted list\n",
    "        # (it is ok for computation since all \"requirement\" nodes are available)\n",
    "        current_node = unlocked_nodes.pop(0)\n",
    "        sorted_nodes.append(current_node)\n",
    "        current_outbound = graph[current_node]['outbound']\n",
    "        if current_outbound is None:\n",
    "            # dead end reached\n",
    "            continue\n",
    "        for node in graph[current_node]['outbound']:\n",
    "            # free the outbound node from requiring current node\n",
    "            graph[node]['inbound'].remove(current_node)\n",
    "            # if the outbound node has no more requirements to be fulfilled,\n",
    "            # unlock it\n",
    "            if len(graph[node]['inbound']) == 0:\n",
    "                unlocked_nodes.append(node)\n",
    "\n",
    "    return sorted_nodes\n",
    "\n",
    "\n",
    "def get_graph_flow(input_nodes):\n",
    "    \"\"\"Build the network graph.\n",
    "\n",
    "    A wrapper function to automate model build.\n",
    "\n",
    "     Args:\n",
    "        input_nodes (:obj:`list` of :obj:`Node`): the input operations of\n",
    "            the graph.\n",
    "\n",
    "     Returns:\n",
    "        requirements_chain (:obj:`list` of :obj:`Node`): a sequence of\n",
    "            operations that ensures computational consistency of the model.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "    connectivity = get_connectivity(input_nodes)\n",
    "    requirements_chain = topological_sort(input_nodes, connectivity)\n",
    "\n",
    "    return requirements_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief explanation of the functions defined in the previous cell:\n",
    "* `get_connectivity` takes a list of input nodes and returns a `dictionary` storing an entry for each `Node` summarizing the connectivity of that `Node`;\n",
    "* `topological_sort` consumes this `dictionary` to produce a `list` of `Node`s starting from the list of input nodes and removing them from their *requirements role for following nodes* (line `69` of previous cell); once a `Node` is freed from all its requirements, it is marked as *unlocked* and add to the list of computable nodes;\n",
    "* `get_graph_flow` is simply a wrapper for the previous two functions.\n",
    "\n",
    "If we extend the `LinearUnitGraph` class with a method `_get_topological_order`, we can verify that the algorithm works as desired on this computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Placeholder object at 0x000002546CE4D6A0>\n",
      "<__main__.Variable object at 0x000002546CE4DA58>\n",
      "<__main__.Variable object at 0x000002546CE4DC50>\n",
      "<__main__.Linear object at 0x000002546CE4DCC0>\n",
      "<__main__.Add object at 0x000002546CE4DBE0>\n"
     ]
    }
   ],
   "source": [
    "class LinearUnitOrder(LinearUnitGraph):\n",
    "    def __init__(self):\n",
    "        LinearUnitGraph.__init__(self)\n",
    "        self.graph = self._get_topological_order()\n",
    "        \n",
    "    def _get_topological_order(self):\n",
    "        return get_graph_flow([self.X, self.W, self.B])\n",
    "        \n",
    "        \n",
    "linearunitorder = LinearUnitOrder()\n",
    "for node in linearunitorder.graph:\n",
    "    print(node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally **bring the graph to life by flowing data through it**.\n",
    "\n",
    "We first load the observed data into the `Placeholder` operation of the graph, then execute a *systolic* sequence of calls to the `forward` methods to all the `Node`s until the final `Node`'s `state` is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:  [[ 0.40074923  0.80374973 -1.76938737]]\n",
      "Prediction:  [[ 2.39164155]]\n"
     ]
    }
   ],
   "source": [
    "def forward_prop(requirements_chain):\n",
    "    \"\"\"Push the current inputs through the whole model.\n",
    "\n",
    "    Consistently complete the systolic sequence of operations.\n",
    "\n",
    "    Args:\n",
    "        requirements_chain (:obj:`list` of :obj:`Node`): a sequence of\n",
    "            operations that ensures computational consistency of the model.\n",
    "\n",
    "    \"\"\"\n",
    "    for node in requirements_chain:\n",
    "        node.forward()\n",
    "\n",
    "\n",
    "class LinearUnit(LinearUnitOrder):\n",
    "    def __init__(self):\n",
    "        LinearUnitOrder.__init__(self)\n",
    "        \n",
    "    def infer(self, x):\n",
    "        self.X.forward(value=x)\n",
    "        forward_prop(self.graph)\n",
    "\n",
    "      \n",
    "linearunit = LinearUnit()\n",
    "\n",
    "x = np.random.randn(1, 3)\n",
    "linearunit.infer(x)\n",
    "print('Observation: ', x)\n",
    "print('Prediction: ', linearunit.s.state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=dataflowbackprop></a>\n",
    "### Reinterpreting the *backpropagation* algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=batch></a>\n",
    "### Batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bostonhousing></a>\n",
    "### The *Boston housing prices* problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
