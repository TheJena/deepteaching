{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - Feedforward models\n",
    "\n",
    "This lesson will present the *dataflow* paradigm and its application to Artificial Neural Networks.\n",
    "All the operations required to create an ANN will be implemented from scratch, and finally used to solve a regression task.\n",
    "\n",
    "### Summary\n",
    "\n",
    "* [Classical visualization of ANNs](#classicalANNs)\n",
    "* [The *dataflow* paradigm](#dataflow)\n",
    "* [Operations](#operations)\n",
    "* [Running a *dataflow* program](#rundataflowprog)\n",
    "* [Reinterpreting the *backpropagation* algorithm](#dataflowbackprop)\n",
    "* [The *Boston housing prices* problem](#bostonhousing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=classicalANNs></a>\n",
    "### Classical visualization of ANNs\n",
    "\n",
    "Our way to think to problems is strongly influenced by the way we visualize them.\n",
    "Artificial Neural Networks visualization has been strongly influenced by biological neural networks.\n",
    "The fundamental processing units, the neurons, are usually depicted as small circles plus their connections.\n",
    "In the picture below, we highlighted a neuron nucleus in gold, its synapsis (i.e. incoming signals channels) in green and its dendrites (i.e. outgoing signals channels) in turquoise.\n",
    "\n",
    "<img src='figures/neuralnet_classic.png', width=360, height=360></img>\n",
    "\n",
    "Although simple, this representation system is highly misleading from a mathematical modelling perspective.\n",
    "In fact, it induces a graph representation in our mind, thinking to neurons' nucleuses as nodes and connections as directed edges.\n",
    "It perfectly depicts the structure of the ensemble of fundamental units, but does not convey much of the **hierarchy of feature maps** that each layer of neurons applies to the preceding layers' features.\n",
    "\n",
    "We can think to the ensemble of the states of the neurons in a particular layer as coordinates of points in a vector space (or its natural extension, a **tensor space**).\n",
    "\n",
    "<img src='figures/neuralnet_classic_hierarchy.png', width=360, height=360></img>\n",
    "\n",
    "First, the ANN *looks* at a point $x \\in X$ using its input layer (the cyan one in figure); the connections between the input layer and the second (hidden) layer then transform this point into a point $\\phi(x) \\in \\Phi$ in what is called a *latent representation* or **feature** (the green one in figure); finally, the connections from the hidden layer to the output layer trasform this feature into a point $y = f(\\phi(x))$ (the magenta one in figure) in the desired output space $Y$.\n",
    "\n",
    "The main change of paradigm in visualization of ANNs is passing from node=neuron/edge=connection graphs to **node=feature map/edge=tensor data structure** graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=dataflow></a>\n",
    "### The *dataflow* paradigm\n",
    "\n",
    "[Dataflow](https://en.wikipedia.org/wiki/Dataflow_programming) is a programming model that thinks to programs as computational graphs, graphs which **nodes are operations** and which **edges contain data that are consumed and produced by the operations** they connect.\n",
    "We will call consumed data *operands* and produced data *results*.\n",
    "\n",
    "Why is the *dataflow* paradigm so important for ANNs research?\n",
    "The main reason is that graph representation of a program exposes important computational optimizations:\n",
    "* **concurrency**, due to the explicit definition of dependencies (edges);\n",
    "* **distributed computing**, thanks to the operations placement which is possible due to the operations modeling (nodes).\n",
    "\n",
    "The degree of concurrency enabled by parallel architectures and parallel programming models played a critical role in the Artificial Neural Networks boom of the last ten years.\n",
    "\n",
    "<img src='figures/google_dl.png', width=480, height=480></img>\n",
    "\n",
    "Mastering the **dataflow programming model** requires to approach the programming problem in four stages.\n",
    "The first three stages compose a **building phase** during which the graph is assembled.\n",
    "The last stage represents the **execution phase**, when the graph is *brought to life* flowing data through it.\n",
    "\n",
    "The four stages are:\n",
    "* **design** an analytical model for you problem;\n",
    "* **identify the transformations** required by the model;\n",
    "* **assemble these tranformations consistently**, i.e. respecting their hierarchical dependencies;\n",
    "* **flow data through the model**, feeding suitable input data to the first transformations and then executing other operations as soon as their required operands become available.\n",
    "\n",
    "To get a grasp on these concepts, we will go step by step through an example.\n",
    "\n",
    "During a data analysis, we **designed a statistical model**, parametric in $\\theta=(b_1, w_1, w_2)$, expressed by the analytical function:\n",
    "\n",
    "$$\\begin{align} f(x, \\theta=(b_1, w_1, w_2)) &= h(x, b_1, w_1)w_2 \\\\ &= \\sigma(\\nu(x)w_1 + b_1)w_2 \\\\ &= \\sigma((x - \\bar{x})w_1 + b_1)w_2 \\end{align}$$\n",
    "\n",
    "This decomposition highlights the dependencies of each stage of computation on preceeding elements.\n",
    "Thus we can proceed to **identify the required elementary transformations** proceeding backward from the last (higher level) operation, iteratively asking **\"Which operands are required to compute this feature?\"**:\n",
    "* the last operation is $f(x, \\theta=(b_1, w_1, w_2)) = h(x, b_1, w_1)w_2$, that it is the product between a vector $h$ and a matrix $w_2$; these operands come from distinct operations;\n",
    "* $w_2$ is a parameter of our model, so it is data that should just be emitted by some *emitting operation* which has no required dependencies; since parameters can be changed, this operation should somehow allow to emit different data depending on the moment it is executed;\n",
    "* $h = h(x, b_1, w_1)$ is itself the otput of an operation $h(x, b_1, w_1) = \\sigma(\\nu(x)w_1 + b_1)$, which requires as operand the quantity $s_1 = \\nu(x)w_1 + b_1$;\n",
    "* $s_1$ is the sum of two operands, namely $\\nu(x)w_1$ and $b_1$;\n",
    "* we can apply to $b_1$ the same reasoning that we applied to $w_2$;\n",
    "* $\\nu(x)w_1$ is the vector-matrix product of the two  operands $\\nu(x)$ and $w_1$;\n",
    "* we can apply to $w_1$ the same reasoning that we applied to $w_2$ and $b_1$;\n",
    "* $\\nu(x) = x - \\bar{x}$ is obtained as the difference betweend the independent operands $x$ and $\\bar(x)$;\n",
    "* $\\bar{x}$ is not a parameter, but an external constant that should be always emitted equal to itself (where a parameter could change);\n",
    "* finally, $x$ is the actual input to the statistical model, the observation of the reality: it should be emitted by an operation which is be fed by the real world.\n",
    "\n",
    "This analysis corresponds to the computational graph depicted below.\n",
    "\n",
    "<img src='figures/dataflow.png', width=480, height=480></img>\n",
    "\n",
    "We see that we have **input nodes** that contain:\n",
    "* data that can be fed in by the real world: **placeholder** operation;\n",
    "* constant information: **constant** operation;\n",
    "* parametric information, that can be changed as required: **variable** operations.\n",
    "\n",
    "Then, we have *real* operations in the form of **internal nodes**:\n",
    "* **sum** node;\n",
    "* **vector-matrix multiplication** nodes;\n",
    "* **sigmoid activation** node.\n",
    "\n",
    "We said before that the intermediate feature spaces represented by ANNs layers actually are vector or tensor spaces.\n",
    "Due to the inherent structure of vector and tensor spaces, points in these spaces are naturally encoded by multidimensional arrays data structures.\n",
    "Thus, **operations** (the circles representing the graph nodes) eventually consume and produce data in the form of **tensors** (the squares on the edges).\n",
    "Usually, the output tensor of the last feature map is not considered as an edge, since it is not consumed by any operation.\n",
    "\n",
    "To simplify the program management, operation placement and data transfer, operations and their output tensor are usually merged into a single implemented object called `Node`.\n",
    "\n",
    "<img src='figures/dataflow_implementation.png', width=480, height=480></img>\n",
    "\n",
    "Such an object should store basically three attributes:\n",
    "* `inbound_nodes`, a list of the **nodes whose output is required by the current node**, i.e. the nodes that provide operands to it;\n",
    "* `outbound_nodes`, a list of **nodes that will use the current node's output**, i.e. the nodes to which it provides operands;\n",
    "* `state`, an array to store **the result of the operation**; the output tensor itself!\n",
    "\n",
    "The node should also have a method:\n",
    "* `forward`, an algorithm to consume the inputs (operands) and produce its output (result).\n",
    "\n",
    "When a `Node` object is created, it should be linked to the `Node`s that contain its required operand.\n",
    "Thus, the `__init__` method of such a class should update the information about the graph's connectivity, taking a `list` of inbound `Node`s and communicating to them that the currently created `Node` requires their state (i.e. updating their `outbound_nodes` attribute).\n",
    "Notice that the connectivity information added by the constructor method of a `Node` is local, since it regards only the edged which have the current `Node` as an extremum.\n",
    "\n",
    "To visualize this, the next figure highlights a subgraph realizing the feature map $s_1 = l_1(x) = \\nu(x)w_1 + b_1$; this subgraph is composed by:\n",
    "* an arbitrary node as the current node (magenta);\n",
    "* its `inbound_nodes` (orange), containing the operands $\\nu(x)$, $w_1$ and $b_1$;\n",
    "* its `outbound_nodes` (sea green), where the output tensor $s_1$ is directed.\n",
    "\n",
    "<img src='figures/dataflow_implementation_Node.png', width=480, height=480></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, inbound_nodes=list()):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        for node in self.inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "        self.outbound_nodes = list()\n",
    "        self.state = None\n",
    "    \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=operations></a>\n",
    "### Operations\n",
    "\n",
    "#### Input operations\n",
    "The basic operations of a *dataflow* graph are input operations.\n",
    "These operations provide *raw data* to the program, data that is not preprocessed or transformed before being fed to the graph.\n",
    "\n",
    "This information comes in three form:\n",
    "* `Constant`, an operation which emits an operand always equal to itself; the value of the operand should be communicated to the operation at its creation;\n",
    "* `Placeholder`, an operation which emits an operand that has been read from the external environment; its `forward` method should thus use a parameter `value` which reads the current observation from the environment;\n",
    "* `Variable`, an operation which emits an operand that must be initialized by the operation's constructor method; this value is stored as the operation's state; the difference from `Constant` operations is that `Variable`s state could change during the model's life.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Constant(Node):\n",
    "    def __init__(self, value):\n",
    "        Node.__init__(self)\n",
    "        self.state = value\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class Placeholder(Node):\n",
    "    def __init__(self):\n",
    "        Node.__init__(self)\n",
    "        \n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.state = value\n",
    "            \n",
    "            \n",
    "class Variable(Node):\n",
    "    def __init__(self, initial_value):\n",
    "        Node.__init__(self)\n",
    "        self.state = initial_value\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear maps and affine transformations\n",
    "\n",
    "**Linear maps** are the most important transformations that can be applied to vector and tensor spaces.\n",
    "Let $X$ and $S$ be two vector spaces, that we will call the *input space* and the *score space* respectively.\n",
    "When both these spaces are finite-dimensional, with dimensions $n$ and $m$ respectively, a linear map\n",
    "\n",
    "$$l: X \\to S$$\n",
    "\n",
    "can be algebraically described by a **vector-matrix multiplication**\n",
    "\n",
    "$$\\begin{gather} xW = \\begin{pmatrix} x_1 & x_2 & \\dots & x_n \\end{pmatrix}\n",
    "                      \\begin{pmatrix} w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "                                      w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "                                      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                                      w_{n1} & w_{n2} & \\dots & w_{nm} \\end{pmatrix}\n",
    "\\end{gather}$$\n",
    "\n",
    "The result of this operation is a vector $s \\in S$ which components are called *scores*\n",
    "\n",
    "$$s_k = \\sum_{i=1}^{n}x_i w_{ik}, k = 1, \\dots m$$\n",
    "\n",
    "The geometric meaning of this transformation is clear: the $k$-th component of $s$ is the result of a projection operation of $x$ onto the one-dimensional subspace of $X$ idetified by the $k$-th column of $W$:\n",
    "\n",
    "$$s_k = \\langle x, W^{(k)} \\rangle, k = 1, \\dots m$$\n",
    "\n",
    "If we identify $X = \\mathbb{R}^n$ and $S = \\mathbb{R}^m$, linear algebra interprets the columns of $W$ as $m$ vectors *sprayed* in $\\mathbb{R}^n$; we could imagine to give each of these vectors a physical interpretation like *cold*, *hot*, *blue* or *red*.\n",
    "A point $x \\in \\mathbb{R}^n$ quantifies an observed objects in terms of numerical coordinates in $\\mathbb{R}^n$; then the dot product $s_k = \\langle x, W^{(k)} \\rangle$ answers to the question *\"how much is this object $x$ cold/hot/blue/red?\"*.\n",
    "The score space $\\mathbb{R}^m$ will thus collect a higher level interpretation of the object point, namely $s = xW$.\n",
    "These kind of abstractions are called **representations**.\n",
    "\n",
    "Linear maps are constrained to represent rotations and homothecies around the origin, as shown by the [polar decomposition](https://en.wikipedia.org/wiki/Polar_decomposition), and consequently cannot account for translations.\n",
    "To model such dependencies, a translation is usually added under the form of a **bias vector** $b \\in S$:\n",
    "\n",
    "$$\\begin{align} l: \\, &X \\to S \\\\ &x \\mapsto xW + b \\end{align}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, x, w):\n",
    "        Node.__init__(self, inbound_nodes=[x, w])\n",
    "        \n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        w = self.inbound_nodes[1].state\n",
    "        self.state = np.dot(x, w)\n",
    "        \n",
    "        \n",
    "class Add(Node):\n",
    "    def __init__(self, s, b):\n",
    "        Node.__init__(self, inbound_nodes=[s, b])\n",
    "        \n",
    "    def forward(self):\n",
    "        s = self.inbound_nodes[0].state\n",
    "        b = self.inbound_nodes[1].state\n",
    "        self.state = s + b\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear activations\n",
    "\n",
    "As stated by the Universal Approximation Theorem, the power of ANNs emerges when they are able to **distort space to extract non-linear features**.\n",
    "\n",
    "In order to achieve this, the output scores of linear transformations $s_k, k=1, \\dots m$ should be filtered by **sigmoid** activations\n",
    "\n",
    "$$\\begin{align} y_k &= \\sigma(s_k) \\\\ &= \\frac{e^{s_k}}{e^{s_k} + 1}\\end{align}$$\n",
    "\n",
    "In practice, many non-linear functions can play the same role.\n",
    "For example, the **hyperbolic tangent**\n",
    "\n",
    "$$\\begin{align} y_k &= \\tanh(s_k) \\\\ &= 2\\sigma(s_k) - 1 \\end{align}$$\n",
    "\n",
    "or the Leaky Rectified Linear Unit (**LeakyReLU**)\n",
    "\n",
    "$$LeakyReLU_{q}(s_k) = \\begin{cases} qs_k, & \\mbox{if } s_k \\leq 0 \\\\ s_k, & \\mbox{if } s_k > 0 \\end{cases}$$\n",
    "\n",
    "where $q \\geq 0$ is a design constant (the case $q = 0$ yields the widespreadly used **ReLU** function).\n",
    "\n",
    "Why do they exist so many activation functions?\n",
    "Should not sigmoid suffice due to the UAT?\n",
    "Although in theory the LeakyReLU and the ReLU functions do not satisfy the hypothesis of the UAT, in real instances they do, and due to their minimal computational complexity with regard to sigmoids and hyperbolic tangents are thus largely used in real models.\n",
    "Nevertheless, sigmoid and hyperbolic tangents are used since they allow more straightforward statistical interpretations:\n",
    "* since $\\sigma(s_k) \\in (0, 1)$, sigmoid units can smoothly approximate boolean behaviour (e.g. the probability that a certain feature has been detected or not);\n",
    "* since $\\tanh(s_k) \\in (-1, 1)$ carries sign information, hyperbolic tangent units can test wether a certain feature is inhibitory or excitatory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "        \n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        e_x = np.exp(x)\n",
    "        self.state = e_x / (1+e_x)\n",
    "        \n",
    "\n",
    "class Tanh(Node):\n",
    "    def __init__(self, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "        \n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        e_2x = np.exp(2*x)\n",
    "        self.state = (e_2x - 1.0) / (e_2x + 1.0)\n",
    "        \n",
    "        \n",
    "class LeakyReLU(Node):\n",
    "    def __init__(self, q, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "        self.q = q\n",
    "        \n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        mask = x > 0\n",
    "        mask = mask + q * (1-mask)\n",
    "        self.state = x * mask\n",
    "        \n",
    "        \n",
    "class ReLU(LeakyReLU):\n",
    "    def __init__(self, x):\n",
    "        LeakyReLU.__init__(self, 0, x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=rundataflowprog></a>\n",
    "### Running a *dataflow* program\n",
    "\n",
    "Up to now, we have developed all the components required to create a computational graph.\n",
    "But before we can run it, we have to accomplish the third stage, **assemble these operations consistently**.\n",
    "We can tackle this stage dividing it into two tasks:\n",
    "* assemble a graph;\n",
    "* automatically detect the computational dependencies between the operations in the graph.\n",
    "\n",
    "As for the assembly, opearations should be chained manually by the programmer.\n",
    "Since he is also the creator of the model, he knows which are the higher level features and which operands (i.e. which lower level abstractions) they require.\n",
    "\n",
    "To test our code, let's assemble the two-layer ANN model we designed above.\n",
    "Suppose that the input $x$ is in $\\mathbb{R}^3$, the intermediate vector space $S$ is $\\mathbb{R}^4$, and the desired response is $y \\in \\mathbb{R}$: we thus require a model with three input units, four hidden neurons and a single output unit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedforwardModelGraph():\n",
    "    def __init__(self):\n",
    "        self.input_nodes = list()\n",
    "        self._build()\n",
    "        \n",
    "    def _build(self):\n",
    "        self.X = Placeholder()\n",
    "        num_hidden = 4\n",
    "        W_1 = Variable(np.random.randn(3, num_hidden))\n",
    "        B_1 = Variable(np.zeros((1, num_hidden)))\n",
    "        s = Add(Linear(self.X, W_1), B_1)\n",
    "        h = Sigmoid(s)\n",
    "        W_2 = Variable(np.random.randn(num_hidden, 1))\n",
    "        self.y = Linear(h, W_2)\n",
    "        self.input_nodes.extend([self.X, W_1, B_1, W_2])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an assembled graph.\n",
    "But the `FeedforwardModelGraph` class still does not contain information on the order of execution of the operations.\n",
    "Consequently, it can not process automatically the calls to the `forward` methods of each node, because it might call them in an inconsistent order!\n",
    "\n",
    "To avoid calling `forward` methods by hand (which is also error prone), we can develop an algorithm to solve this problem automatically.\n",
    "\n",
    "Remember that the constructor methods of the `Node`s take care of storing local connectivity information.\n",
    "Since all `Node` objects in a graph store local connectivity information, we can use it to reconstruct the global topology of the graph and to extract dependencies.\n",
    "Suppose the computational graph is composed by nodes $O_1, O_2, \\dots O_p$.\n",
    "We would like a permutation $O_{i_1}, O_{i_2}, \\dots O_{i_p}$ such that all `inbound_nodes` of node $O_{i_{\\bar{j}}}$ have indices $i_j$ such that $j < \\bar{j}$.\n",
    "This problem is known as **topological sorting**.\n",
    "\n",
    "There are many ways to compute the topological sorting of a graph given its connectivity information.\n",
    "We use [Kahn's algorithm](https://en.wikipedia.org/wiki/Topological_sorting#Algorithms) for our implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_connectivity(input_nodes):\n",
    "    \"\"\"Create a description of the connections of each node in the graph.\n",
    "\n",
    "    Recurrent connections (i.e. connections of a node with itself) are excluded.\n",
    "\n",
    "    Args:\n",
    "        input_nodes (:obj:`list` of :obj:`Node`): the input operations of\n",
    "            the model.\n",
    "\n",
    "    Returns:\n",
    "        graph (:obj:`dict` of :obj:`dict` of :obj:`set`): a description of the\n",
    "            graph's connectivity in terms of inbound-outbound nodes of each\n",
    "            node.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    graph = dict()\n",
    "    nodes = input_nodes.copy()\n",
    "    while len(nodes) != 0:\n",
    "        # select a node\n",
    "        current_node = nodes.pop(0)\n",
    "        # if no information has been collected yet, set up dict entry\n",
    "        if current_node not in graph:\n",
    "            graph[current_node] = {'inbound': set(), 'outbound': set()}\n",
    "        # scroll through current node's outbound nodes\n",
    "        for node in current_node.outbound_nodes:\n",
    "            # if no information has been collected yet, set up dict entry\n",
    "            if node not in graph:\n",
    "                nodes.append(node)\n",
    "                graph[node] = {'inbound': set(), 'outbound': set()}\n",
    "            # add reciprocal connectivity information\n",
    "            graph[current_node]['outbound'].add(node)\n",
    "            graph[node]['inbound'].add(current_node)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def topological_sort(input_nodes, graph):\n",
    "    \"\"\"Get a consistent sequence of operations on the given graph.\n",
    "\n",
    "    Args:\n",
    "        input_nodes (:obj:`list` of :obj:`Node`): the input operations of\n",
    "            the model.\n",
    "        graph (:obj:`dict` of :obj:`dict` of :obj:`set`): a description of the\n",
    "            graph's connectivity.\n",
    "\n",
    "    Returns:\n",
    "        sorted_nodes (:obj:`list` of :obj:`Node`): a sequence of operations\n",
    "            that ensures computational consistency of the model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sorted_nodes = list()\n",
    "    unlocked_nodes = input_nodes.copy()\n",
    "    while len(unlocked_nodes) != 0:\n",
    "        # select an inbound-free node and add it the sorted list\n",
    "        # (it is ok for computation since all \"requirement\" nodes are available)\n",
    "        current_node = unlocked_nodes.pop(0)\n",
    "        sorted_nodes.append(current_node)\n",
    "        current_outbound = graph[current_node]['outbound']\n",
    "        if current_outbound is None:\n",
    "            # dead end reached\n",
    "            continue\n",
    "        for node in graph[current_node]['outbound']:\n",
    "            # free the outbound node from requiring current node\n",
    "            graph[node]['inbound'].remove(current_node)\n",
    "            # if the outbound node has no more requirements to be fulfilled,\n",
    "            # unlock it\n",
    "            if len(graph[node]['inbound']) == 0:\n",
    "                unlocked_nodes.append(node)\n",
    "\n",
    "    return sorted_nodes\n",
    "\n",
    "\n",
    "def get_graph_flow(input_nodes):\n",
    "    \"\"\"Build the network graph.\n",
    "\n",
    "    A wrapper function to automate model build.\n",
    "\n",
    "     Args:\n",
    "        input_nodes (:obj:`list` of :obj:`Node`): the input operations of\n",
    "            the graph.\n",
    "\n",
    "     Returns:\n",
    "        requirements_chain (:obj:`list` of :obj:`Node`): a sequence of\n",
    "            operations that ensures computational consistency of the model.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "    connectivity = get_connectivity(input_nodes)\n",
    "    requirements_chain = topological_sort(input_nodes, connectivity)\n",
    "\n",
    "    return requirements_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief explanation of the functions defined in the previous cell:\n",
    "* `get_connectivity` takes a list of input nodes and returns a `dictionary` storing an entry for each `Node` summarizing the connectivity of that `Node`;\n",
    "* `topological_sort` consumes this `dictionary` to produce a `list` of `Node`s starting from the list of input nodes and removing them from their *requirements role for following nodes* (line `69` of previous cell); once a `Node` is freed from all its requirements, it is marked as *unlocked* and add to the list of computable nodes;\n",
    "* `get_graph_flow` is simply a wrapper for the previous two functions.\n",
    "\n",
    "If we extend the `FeedforwardModelGraph` class with a method `_get_topological_order`, we can verify that the algorithm works as desired on this computational graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Placeholder object at 0x0000015A3DE00C50>\n",
      "<__main__.Variable object at 0x0000015A3DE00358>\n",
      "<__main__.Variable object at 0x0000015A3DE00DD8>\n",
      "<__main__.Variable object at 0x0000015A3DE00EB8>\n",
      "<__main__.Linear object at 0x0000015A3DE00E10>\n",
      "<__main__.Add object at 0x0000015A3DE00E48>\n",
      "<__main__.Sigmoid object at 0x0000015A3DE00E80>\n",
      "<__main__.Linear object at 0x0000015A3DE00EF0>\n"
     ]
    }
   ],
   "source": [
    "class FeedforwardModelSorted(FeedforwardModelGraph):\n",
    "    def __init__(self):\n",
    "        FeedforwardModelGraph.__init__(self)\n",
    "        self.graph = self.get_topological_order()\n",
    "        \n",
    "    def get_topological_order(self):\n",
    "        return get_graph_flow(self.input_nodes)\n",
    "        \n",
    "        \n",
    "ff_model_sorted = FeedforwardModelSorted()\n",
    "\n",
    "for node in ff_model_sorted.graph:\n",
    "    print(node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally **bring the graph to life by flowing data through it**.\n",
    "\n",
    "We first load the observed data into the `Placeholder` operation of the graph, then execute a *systolic* sequence of calls to the `forward` methods to all the `Node`s until the final `Node`'s `state` is computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(requirements_chain):\n",
    "    \"\"\"Push the current inputs through the whole model.\n",
    "\n",
    "    Consistently complete the systolic sequence of operations.\n",
    "\n",
    "    Args:\n",
    "        requirements_chain (:obj:`list` of :obj:`Node`): a sequence of\n",
    "            operations that ensures computational consistency of the model.\n",
    "\n",
    "    \"\"\"\n",
    "    for node in requirements_chain:\n",
    "        node.forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed:  [[ 1.74422798  0.82719048 -1.92413138]]\n",
      "Predicted:  [[ 3.24496326]]\n"
     ]
    }
   ],
   "source": [
    "class FeedforwardModelInference(FeedforwardModelSorted):\n",
    "    def __init__(self):\n",
    "        FeedforwardModelSorted.__init__(self)\n",
    "        \n",
    "    def infer(self, x):\n",
    "        self.X.forward(value=x)\n",
    "        forward_prop(self.graph)\n",
    "        print('Observed: ', x)\n",
    "        print('Predicted: ', ff_model_inference.y.state)\n",
    "\n",
    "      \n",
    "ff_model_inference = FeedforwardModelInference()\n",
    "\n",
    "x = np.random.randn(1, 3)\n",
    "ff_model_inference.infer(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=dataflowbackprop></a>\n",
    "### Reinterpreting the *backpropagation* algorithm\n",
    "\n",
    "In the first lesson, we reformulated the learning problem in terms of dynamical systems.\n",
    "In fact, given a parametric model\n",
    "\n",
    "$$\\mathcal{F}_{\\Theta} = \\{f_{\\theta}: X \\to Y\\}_{\\theta \\in \\Theta}$$\n",
    "\n",
    "and a loss functional\n",
    "\n",
    "$$L: \\mathcal{F}_{\\Theta} \\to \\mathbb{R}$$\n",
    "\n",
    "we can use $L$ as a *torchlight* to evaluate a point $f_{\\theta_{i}} \\in \\mathcal{F}_{\\Theta}$ and take a step to get to a (hopefully) better point $f_{\\theta_{i+1}}$.\n",
    "This procedure can be iterated to yield a discrete sequence of approximations\n",
    "\n",
    "$$f_{\\theta_{0}}, f_{\\theta_{1}}, f_{\\theta_{2}}, \\dots f_{\\theta_{i}} \\dots$$\n",
    "\n",
    "What is still missing is how to use $L$ to get the sequence $\\theta_{0}, \\theta_{1}, \\theta_{2}, \\dots \\theta_{i} \\dots$ or, equivalently, the sequence of updates\n",
    "\n",
    "$$\\begin{align} \\Delta \\theta_{1} &= \\theta_{1}-\\theta_{0} \\\\\n",
    "                \\Delta \\theta_{2} &= \\theta_{2}-\\theta_{1} \\\\\n",
    "                \\dots \\\\\n",
    "                \\Delta \\theta_{i} &= \\theta_{i}-\\theta_{i-1} \\\\\n",
    "                \\dots\n",
    "\\end{align}$$\n",
    "\n",
    "When the parametric model is implemented as an ANN, the *backpropagation* algorithm generates the sequence $\\{\\Delta \\theta_{i}\\}_{i \\in \\mathbb{N}}$ in a two-steps cycle:\n",
    "* first, an approximation of the loss functional called the **empirical loss** is used to evaluate the performance of the current approximation $f_{\\theta_{i-1}}$;\n",
    "* second, the gradient descent algorithm (or more sophisticated variants of its) is used to compute an approximation of $\\nabla_{\\theta}L$ and improve the performance of the model taking a step in the direction $\\Delta \\theta_{i}$ that should minimize the loss.\n",
    "\n",
    "In statistical learning theory, the loss functional evaluated on a model $\\mathcal{F}_{\\Theta}$ is usually defined as a probability-weighted mean of the error that an approximation $f_{\\theta_{i}}$ makes when it is used to approximate the real (and unknown) map $f(x) = \\hat{y}$:\n",
    "\n",
    "$$L(f_{\\theta}) = \\int_{X} l(\\hat{y}, f_{\\theta}(x)) \\mathbb{P}(dx)$$\n",
    "\n",
    "Here, $l: Y \\times Y \\to \\mathbb{R}_0^+$ is a non-negative distance function defined on the output space $Y$, that should evaluate to zero whenever the prediction is correct ($f_{\\theta}(x) = \\hat{y}$) and to positive numbers otherwise: this function is called **loss function**.\n",
    "\n",
    "Clearly, not all points $x \\in X$ are known at training time; moreover, some of them might never be seen by the ANNs also during inference!\n",
    "Thus, the formula above is practically untractable.\n",
    "To solve this issue, the concept of *empirical loss* comes into play.\n",
    "In fact, an extreme approximation of the analytical weighted mean is\n",
    "\n",
    "$$\\begin{align} L(f_{\\theta}) &\\approx \\tilde{L}(f_{\\theta}) \\\\\n",
    "                             &= l(\\hat{y}, f_{\\theta}(x))\n",
    "\\end{align}$$\n",
    "\n",
    "i.e. the evaluation of $f_{\\theta}$ on just a single data point!\n",
    "Although highly imprecise, this computation is certainly practical.\n",
    "\n",
    "In the previous section, we built a set of components that allow us to write parametric models $f_{\\theta}$ and also to evaluate them on observable variables $x \\in X$.\n",
    "It is thus straightforward to extend the *dataflow* model to create **loss operations**.\n",
    "Loss operations should consume the output of the last inference operation and a placeholder operation emitting the correct label $\\hat{y}$, and should produce a real non-negative number representing the value of the loss function on this pair.\n",
    "\n",
    "For example, for regression tasks we could implement the Mean Squared Error (MSE) loss function:\n",
    "\n",
    "$$l(\\hat{y}, f_{\\theta}(x)) = \\frac{1}{2}(\\hat{y} - f_{\\theta}(x))^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y_hat, y):\n",
    "        Node.__init__(self, inbound_nodes=[y_hat, y])\n",
    "        \n",
    "    def forward(self):\n",
    "        y_hat = self.inbound_nodes[0].state\n",
    "        y = self.inbound_nodes[1].state\n",
    "        delta = y_hat - y\n",
    "        self.state = np.sum(np.square(delta)) / 2.0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the loss is not a necessary step for parametric learning.\n",
    "It is nevertheless a fundamental step for the *backpropagation* algorithm, which goal is exactly to generate the updates to reduce the loss of the model.\n",
    "\n",
    "As stated in the previous lesson, the term *backpropagation* refers to communicating error signals back through an ANN.\n",
    "But what is exactly an error signal?\n",
    "And how is the loss function useful to this goal?\n",
    "\n",
    "Basic calculus introduces the concept of derivative of a function\n",
    "\n",
    "$$f'(x) = \\frac{df}{dx}$$\n",
    "\n",
    "to express which is the expected variation $df$ of function $f$ evaluated at $x$ if we take a positive and arbitrarily small step $dx$ moving towards $+\\infty$.\n",
    "If $f'(x) > 0$, taking such a step $dx$ means that the dependent variable $y = f(x)$ will increase; conversely, if $f'(x) < 0$ and we take such a step towards $+\\infty$, then $y = f(x)$ will decrease; finally, when $f'(x) = 0$ we interpret this as the fact that $f$ is *locally insensitive* to changes of $x$, i.e. the independent variable has little or no effect on the dependent one.\n",
    "\n",
    "Notice now that the derivative is composed by a *form* and by *instances*: $f'(\\cdot)$ is just a *formal way to process* any point $x$ belonging to $f'$'s domain, and takes numerical meanining only when it is *instantiated*, i.e. computed at a specific point $x$.\n",
    "We call the numerical value of the derivative of a function $f$ (or its high-dimensional counterpart, the gradient) computed at a specific point $\\tilde{x}$ the **error signal** directed from $f$ to $x$:\n",
    "\n",
    "$$\\nabla_{x}f \\rvert_{\\tilde{x}} = \\left(\\frac{\\partial f}{\\partial x_i} \\bigg\\rvert_{\\tilde{x_{i}}} \\right)_{i=1, 2, \\dots n}$$\n",
    "\n",
    "The geometric interpretation of the error signal is thus the same of the gradient.\n",
    "Suppose we are in a point $x \\in X$ and we are constrained to take a step, only one step, of fixed length $\\|\\Delta x\\|$.\n",
    "We would like to take this step $\\Delta x$ such that $f(x + \\Delta x) - f(x)$ is maximum.\n",
    "The direction $\\nabla_{x}f$ is the one with this desired property, while its opposed direction $-\\nabla_{x}f$ yields a variation $f(x + \\Delta x) - f(x)$ which is minimum.\n",
    "But whenever $f$ is a non-linear function, the gradient hold this desirable property only in a local neighborhood of $x$: in fact, for non-linear functions, in general $\\nabla_{x}f \\rvert_{\\tilde{x}_1} \\neq \\nabla_{x}f \\rvert_{\\tilde{x}_2}$ when $\\tilde{x}_1 \\neq \\tilde{x}_2$.\n",
    "The family of Gradient Descent algorithms from numerical optimization is particularly concerned with the search of good strategies to choose steplength parameters $\\{\\alpha_{i}\\}_{i \\in \\mathbb{N}}$ such that in practice the local descent directions $\\{\\nabla_{x}f \\rvert_{\\tilde{x}_{i}}\\}_{i \\in \\mathbb{N}}$ are only followed in a neighborhood where the descent property holds.\n",
    "\n",
    "In the parametric statistical learning, these calculus and numerical optimization methods can be used treating the parameters $\\theta \\in \\Theta$ as variables and the loss function $l$ as the objective function to be minimized.\n",
    "The direction of steepest ascent can in fact be computed as:\n",
    "\n",
    "$$\\nabla_{\\theta}l \\rvert_{\\tilde{\\theta}} = \\left(\\frac{\\partial l}{\\partial \\theta_{i}} \\bigg \\rvert_{\\tilde{\\theta_{i}}}\\right)_{i = 1, 2, \\dots n}$$\n",
    "\n",
    "and the movement in the *parameter state space* (remember that we interpret the learning procedure as a discrete time dynamical system) can be computed as:\n",
    "\n",
    "$$\\Delta \\theta = -\\alpha \\nabla_{\\theta}l \\rvert_{\\tilde{\\theta}}$$\n",
    "\n",
    "In Deep Learning research, the steplength $\\alpha$ is called *learning rate* and is sometimes denoted with $\\eta$.\n",
    "\n",
    "Two points are still missing from our analysis of *backpropagation*:\n",
    "* how to actually compute errors through a two-or-more layers model;\n",
    "* how to implement the procedure in the *dataflow* paradigm.\n",
    "\n",
    "In order to get a better grasp on visualizing *backpropagation*, let's look again at the defined computational graph.\n",
    "\n",
    "<img src='figures/dataflow_implementation_errorsignals.png', height=480, width=480></img>\n",
    "\n",
    "The red squares represent error signals that should communicate to previous `Node`s as their `state` should change in order to alter the ultimate output $y = f(x)$ of the model, while the red arrows represent to which `Node` each signal should be communicated to.\n",
    "Notice that no error should be communicated to `Node`s which `state` cannot be modified (`Constant`) or on which the model itself has no control (`Placeholder`, since their information comes from the external world).\n",
    "\n",
    "The first problem is: *\"how should the output $y = f(x)$ change? Should it increase? Or should it decrease?\"*.\n",
    "The answer to this question is the loss operation itself.\n",
    "As for the Perceptron or Adaline, the agent should be able to determine if its response $y$ should increase (when the last response $y$ was such that $\\hat{y} > y$) or if its response should decrease (when $\\hat{y} < y$).\n",
    "We can use the information coming from $l(\\hat{y}, y)$ to decide wether to increase or decrease $y$, which in turn will communicate its modifiable parameters (`Variable` operations) how to get this desired change.\n",
    "This first information comes out as\n",
    "\n",
    "$$\\delta y = \\frac{\\partial l}{\\partial y} \\bigg \\rvert_{\\tilde{y}}$$\n",
    "\n",
    "We now have a *desired behaviour* for $y$.\n",
    "We can obtain this desired behaviour by altering the modifiable part of the model, **translating the desired change on $y$ on a desired change on the operands of the operation that produced $y$**.\n",
    "\n",
    "In our example, pick the `Node` whose output is $y$ and determine how the result, $y$ itself, is changed by each component of the operands:\n",
    "\n",
    "$$\\begin{align} \\frac{\\partial y}{\\partial h_k} &= W_k^2 \\\\\n",
    "                \\frac{\\partial y}{\\partial W_k^2} &= h_k\n",
    "\\end{align}$$\n",
    "\n",
    "that yields gradients:\n",
    "\n",
    "$$\\begin{align} \\nabla_{h}y \\rvert_{\\tilde{h}} &= \\tilde{W}^2 \\\\\n",
    "                \\nabla_{W^2}y \\rvert_{\\tilde{W}^2} &= \\tilde{h}\n",
    "\\end{align}$$\n",
    "\n",
    "Notice that this is not yet a translation of the desired behaviour for $y$, represented by $\\frac{\\partial l}{\\partial y}$.\n",
    "To translate this into a change on $h$ and $W^2$, we have to **compose the behaviour of $y$ with the influence $h$ and $W^2$ have on it using the chain rule**:\n",
    "\n",
    "$$\\begin{align} \\delta h &= \\nabla_{h}l \\rvert_{\\tilde{h}} \\\\\n",
    "                         &= \\nabla_{y}l \\rvert_{\\tilde{y}} \\nabla_{h}y \\rvert_{\\tilde{h}} \\\\\n",
    "                \\delta W^2 &= \\nabla_{W^2}l \\rvert_{\\tilde{W}^2} \\\\\n",
    "                           &= \\nabla_{y}l \\rvert_{\\tilde{y}} \\nabla_{W^2}y \\rvert_{\\tilde{h}}\n",
    "\\end{align}$$\n",
    "\n",
    "Analogous procedures for $b$ and $W^1$ yield:\n",
    "\n",
    "$$\\begin{align} \\delta b &= \\nabla_{b}l \\rvert_{\\tilde{b}} \\\\\n",
    "                         &= \\nabla_{y}l \\rvert_{\\tilde{y}} \\nabla_{h}y \\rvert_{\\tilde{h}} \\nabla_{s}h \\rvert_{\\tilde{s}} \\nabla_{b}s \\rvert_{\\tilde{b}}\\\\\n",
    "                \\delta W^1 &= \\nabla_{W^1}l \\rvert_{\\tilde{W}^1} \\\\\n",
    "                           &= \\nabla_{y}l \\rvert_{\\tilde{y}} \\nabla_{h}y \\rvert_{\\tilde{h}} \\nabla_{s}h \\rvert_{\\tilde{s}} \\nabla_{W^1}s \\rvert_{\\tilde{W}^1}\n",
    "\\end{align}$$\n",
    "\n",
    "where we followed the chain rule a bit longer.\n",
    "It is also clear where the name backpropagation comes from.\n",
    "We are ultimately interested in the error signals, and to get them we composed the gradients starting from the loss operation and moving *backwards* through the operations.\n",
    "If we look at the figure above, it's just like if we had flipped the computational graph and *composed the error operations in a backward sense*.\n",
    "\n",
    "This observation leads immediately to an idea for the implementation.\n",
    "\n",
    "We should add to each `Node` a second method (an algorithm) called `backward` that should consume an `incoming_error` signal (and eventually other needed operands, as we will see) and produce a `outgoing_errors` tensors directed to each of its operand `Node`s.\n",
    "We will implement `incoming_error` as an array of the same shape as `state`, and `outgoing_errors` as a `dictionary` of arrays which shapes should be identical to the shape of the `state`s of the operation towards which are directed.\n",
    "Each `Node` should be responsible of collecting all the error signals directed to it from its `oubound_nodes` before consuming it through the `backward` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=list()):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        for node in self.inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "        self.outbound_nodes = list()\n",
    "        self.state = None\n",
    "        self.incoming_error = None\n",
    "        self.outgoing_errors = dict()\n",
    "        self.is_trainable = False\n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self):\n",
    "        raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input operations don't need to transmit an error signal to any `Node`, and thus should limit to `pass`, if their state cannot be changed by the agent (`Constant`s and `Placeholder`s) or to collect error signals from operations that use their state (in the case of `Variable` operations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Constant(Node):\n",
    "    def __init__(self, value):\n",
    "        Node.__init__(self)\n",
    "        self.state = value\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def backward(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class Placeholder(Node):\n",
    "    def __init__(self):\n",
    "        Node.__init__(self)\n",
    "        \n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.state = value\n",
    "            \n",
    "    def backward(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class Variable(Node):\n",
    "    def __init__(self, initial_value):\n",
    "        Node.__init__(self)\n",
    "        self.state = initial_value\n",
    "        self.is_trainable = True\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        self.incoming_error = np.zeros_like(self.state)\n",
    "        for node in self.outbound_nodes:\n",
    "            self.incoming_error += node.outgoing_errors[self]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we added a Boolean attribute `is_trainable` that will be used by the `deepteaching` framework to identify which operations contain trainable parameters.\n",
    "\n",
    "We will now show how to think, interpret and finally implement backpropagation for linear transformation.\n",
    "The same reasonings can be applied with slight modifications to all the used operations, but we will not focus much on other kind of operations, and we will only point out the main characteristics.\n",
    "\n",
    "We said before that a linear transformation is a map\n",
    "\n",
    "$$l: \\mathbb{R}^n \\to \\mathbb{R}^m$$\n",
    "\n",
    "coded by $xW = s$.\n",
    "The output $s$ thus depends on two operands: the input point $x$ and the matrix $W$.\n",
    "We will name the columns $W^{(k)}$ of $W$ **filters**, and we remeber now that we interpret filters as questions that the operation $l$ poses to its inputs, such as *\"how much is $x$ cold/hot/blue/red?\"*.\n",
    "\n",
    "We will first show how to interpret and compute $\\delta x$, and secondly how to interpret and compute $\\delta W$.\n",
    "\n",
    "To compute $\\delta x$, we saw that backpropagation needs the Jacobian\n",
    "\n",
    "$$\\bigg( \\frac{\\partial s_k}{\\partial x_i} \\bigg)_{i=1,2,\\dots n, k=1,2,\\dots m} = W$$\n",
    "\n",
    "to translate $\\delta s$ in a change $\\delta x$\n",
    "Notice that the Jacobian is $W$ itself, and that we can interpret its rows $W_{(i)} = \\vec{e}_i W$ as the answers to the questions for the *elementary objects* of $\\mathbb{R}^n$ represented by the canonical basis $\\vec{e}_i, i = 1, 2, \\dots n$.\n",
    "\n",
    "If we want to know how to alter $x$ in a proper way, we can decompose this search on each component since $\\delta x = (\\delta x_i)_{i=1,2,\\dots n}$.\n",
    "In fact, changing $x_i$ perturbs only the answer $W_{(i)}$ to $\\vec{e}_i$: how can we measure the relation between $\\delta s$ and $\\delta x_i$?\n",
    "The dot product\n",
    "\n",
    "$$\\delta x_i = \\langle \\delta s, W_{(i)} \\rangle$$\n",
    "\n",
    "is a measure of the alignment between $\\delta s$ and $l(\\vec{e}_i)$.\n",
    "For example, if $\\delta x_i$ > 0, changing the input pattern $x$ by a $\\Delta x$ such that $\\Delta x_i > 0$ will in fact *exalt* $W_{(i)}$, and since $W_{(i)}$ is aligned with $\\delta s$, this change will ultimately move $s$ in the desired direction.\n",
    "\n",
    "The correct implementation is simply\n",
    "\n",
    "$$\\delta x = \\delta s W^T$$\n",
    "\n",
    "since $\\delta s W^T = (\\langle \\delta s, W_{(i)} \\rangle_{i=1,2,\\dots n}$.\n",
    "\n",
    "<img src='figures/delta_x.png', width=640, height=640></img>\n",
    "\n",
    "Now, we will focus on $\\delta W$.\n",
    "Where the goal of $\\delta x$ is for the input $x$ to change in order to answer the **fixed questions** in a better way, the goal of $\\delta W$ is to pose **different questions** to data.\n",
    "\n",
    "We should now introduce the concept of **tensor derivative**, an extension of the Jacobian.\n",
    "Since $\\delta s$ is a vector in $\\mathbb{R}^m$ and $W$ is a matrix in $\\mathcal{M}_n^m(\\mathbb{R})$, the quantity\n",
    "\n",
    "$$\\bigg( \\frac{\\partial s_k}{\\partial W_{i\\tilde{k}}} \\bigg)_{k=1,2,\\dots m}^{i=1,2,\\dots n,\\tilde{k}=1,2,\\dots m}$$\n",
    "\n",
    "can be thought of as a *vector of matrices*, whose $k$-th element contains a $n$-by-$m$ matrix.\n",
    "We now point out that\n",
    "\n",
    "$$\\frac{\\partial s_k}{\\partial W_{i\\tilde{k}}} = \\delta_{k}^{\\tilde{k}} x_i$$\n",
    "\n",
    "since the $k$-th unit of the score $s_k$ depends only on the $k$-th column of $W$, i.e. on the $k$-th question.\n",
    "For this reason, almost every element in the matrix at place $k$ in the tensor derivative will be zero (plotted in light grey in the following image).\n",
    "\n",
    "<img src='figures/delta_w.png', width=640, height=640></img>\n",
    "\n",
    "We can thus intepret each component matrix as a *patch* to be applied to $W$ in order to slightly change the $k$-th question, making it more similar to the input $x$.\n",
    "The effect of this patch should be combined with the incoming error component $\\delta s_k$ to make the filter look for **$x$-iness** or **non-$x$-iness**, depending on $\\delta s_k$ being positive or negative.\n",
    "\n",
    "Suppose $U$ is a matrix of the same shape as $W$ and which columns $U^{(k)} = x^T$ are all equal to the input pattern $x$.\n",
    "Suppose $T$ is a matrix of the same shape as $W$ and which rows $T_{(i)} = \\delta s$ are all equal to the incoming error signal $\\delta s$.\n",
    "Then $\\delta W$ can be obtained using the Hadamard product\n",
    "\n",
    "$$\\delta W = T \\odot U$$\n",
    "\n",
    "The implementation is straightforward using `numpy`'s [broadcasting](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, x, W):\n",
    "        Node.__init__(self, inbound_nodes=[x, W])\n",
    "        \n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        W = self.inbound_nodes[1].state\n",
    "        self.state = np.dot(x, W)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.incoming_error = np.zeros_like(self.state)\n",
    "        for node in self.outbound_nodes:\n",
    "            self.incoming_error += node.outgoing_errors[self]\n",
    "        W = self.inbound_nodes[1].state\n",
    "        delta_x = np.dot(self.incoming_error, W.T)\n",
    "        x = self.inbound_nodes[0].state\n",
    "        delta_W = self.incoming_error * x.T\n",
    "        self.outgoing_errors[self.inbound_nodes[0]] = delta_x\n",
    "        self.outgoing_errors[self.inbound_nodes[1]] = delta_W\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of translation, or vector additions, is far simpler.\n",
    "In fact, letting $v = x+b$ we have\n",
    "\n",
    "$$\\begin{align} \\frac{\\partial v_k}{\\partial x_i} \\bigg \\rvert_{\\tilde{x}} &= \\delta_i^k \\\\\n",
    "                \\frac{\\partial v_k}{\\partial b_j} \\bigg \\rvert_{\\tilde{b}} &= \\delta_j^k\n",
    "\\end{align}$$\n",
    "\n",
    "and thus error signals\n",
    "\n",
    "$$\\begin{align} \\delta x &= \\delta s \\\\\n",
    "                \\delta b &= \\delta s\n",
    "\\end{align}$$\n",
    "\n",
    "I.e., the `backward` method assigns equal responsibility for the error to both operands, and simply asks both to change their state in order to improve the final answer $y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, x, bias):\n",
    "        Node.__init__(self, inbound_nodes=[x, bias])\n",
    "    \n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        b = self.inbound_nodes[1].state\n",
    "        self.state = x + b\n",
    "        \n",
    "    def backward(self):\n",
    "        self.incoming_error = np.zeros_like(self.state)\n",
    "        for node in self.outbound_nodes:\n",
    "            self.incoming_error += node.outgoing_errors[self]\n",
    "        delta_x = self.incoming_error\n",
    "        delta_b = self.incoming_error\n",
    "        self.outgoing_errors[self.inbound_nodes[0]] = delta_x\n",
    "        self.outgoing_errors[self.inbound_nodes[1]] = delta_b\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to design the `backward` method for activation functions.\n",
    "\n",
    "The sigmoid function and the hyperbolic tangent can be treated in a similar way, since we saw earlier that $\\sigma(s) = 2\\tanh(s)-1$.\n",
    "Suppose $\\sigma$ acts as a non-linear distortion $y_k = \\sigma(s_k), k = 1, 2, \\dots m$ on a Euclidean space $\\mathbb{R}^m$.\n",
    "The Jacobian\n",
    "\n",
    "$$\\nabla_{s}y \\rvert_{\\tilde{s}} = \\bigg( \\frac{\\partial y_j}{\\partial s_i} \\bigg)_{i,j=1,2,\\dots m}$$\n",
    "\n",
    "is diagonal, since **each activation is independent from the others**.\n",
    "**This property is common to all activation funcitons**, and thus composing this Jacobian with the incoming error signal $\\delta y$ has two equivalent forms:\n",
    "\n",
    "$$\\begin{align} \\delta s &= \\delta y \\nabla_{s}y \\rvert_{\\tilde{s}} \\\\\n",
    "                \\delta s &= \\delta y \\odot (\\frac{\\partial y_k}{\\partial s_k})_{k=1,2,\\dots m}\n",
    "\\end{align}$$\n",
    "\n",
    "where the latter is way more efficient than the former.\n",
    "To implement the `bacward` method, it is sufficient to note that\n",
    "\n",
    "$$\\sigma'(s) = \\sigma(s) (1-\\sigma(s))$$\n",
    "\n",
    "An important point should be noted here: due to the symmetry of $\\sigma$ with respect to the point $(0, \\frac{1}{1})$, the maximum value of this derivative is $\\frac{1}{4}$; consequently, the maximum value of $\\tanh'(s)$ is $\\frac{1}{2}$.\n",
    "This means that each component of the incoming error $\\delta y$ is attenuated by at least a factor four: if we suppose to stack many activation layers composed by sigmoids or hyperbolic tangent functions, the error signal will eventually vanish.\n",
    "This problem is known as the **vanishing gradient** problem, and is particularly important for models which are very *deep* in the sense that error signals should be propagated back through many operations.\n",
    "\n",
    "This argument, together with computational efficiency, is one of the main reason why LeakyReLUs are preferred in real models.\n",
    "As for backpropagation through LeakyReLU functions, elementary calculus shows that\n",
    "\n",
    "$$LeakyReLU_{q}'(s) = \\begin{cases} q, & \\mbox{if } s \\leq 0 \\\\ 1, & \\mbox{if } s > 0 \\end{cases}$$\n",
    "\n",
    "and thus\n",
    "\n",
    "$$\\delta s = \\begin{cases} q \\delta y, & \\mbox{if } s \\leq 0 \\\\ \\delta y, & \\mbox{if } s > 0 \\end{cases}$$\n",
    "\n",
    "This can be interpreted as the error being left invariated when it's related to positive scores, and attenuated only when it's related to negative scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "\n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        e_x = np.exp(x)\n",
    "        self.state = e_x / (1+e_x)\n",
    "\n",
    "    def backward(self):\n",
    "        self.incoming_error = np.zeros_like(self.state)\n",
    "        for node in self.outbound_nodes:\n",
    "            self.incoming_error += node.outgoing_errors[self]\n",
    "        self.outgoing_errors[self.inbound_nodes[0]] = self.state * (1-self.state) * self.incoming_error\n",
    "\n",
    "        \n",
    "class Tanh(Node):\n",
    "    def __init__(self, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "\n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        e_2x = np.exp(2*x)\n",
    "        self.state = (e_2x-1) / (e_2x+1)\n",
    "\n",
    "    def backward(self):\n",
    "        self.incoming_error = np.zeros_like(self.state)\n",
    "        for node in self.outbound_nodes:\n",
    "            self.incoming_error += node.outgoing_errors[self]\n",
    "        self.outgoing_errors[self.inbound_nodes[0]] = (1+self.state) * (1-self.state) * self.incoming_error\n",
    "\n",
    "\n",
    "class LeakyReLU(Node):\n",
    "    def __init__(self, q, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        mask = x > 0\n",
    "        mask = mask + self.q * (1-mask)\n",
    "        self.state = x * mask\n",
    "\n",
    "    def backward(self):\n",
    "        self.incoming_error = np.zeros_like(self.state)\n",
    "        for node in self.outbound_nodes:\n",
    "            self.incoming_error += node.outgoing_errors[self]\n",
    "        x = self.inbound_nodes[0].state\n",
    "        mask = x > 0\n",
    "        mask = mask + self.q * (1-mask)\n",
    "        self.outgoing_errors[self.inbound_nodes[0]] = mask * self.incoming_error\n",
    "\n",
    "\n",
    "class ReLU(LeakyReLU):\n",
    "    def __init__(self, x):\n",
    "        LeakyReLU.__init__(self, 0, x)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "\n",
    "    def forward(self):\n",
    "        x = self.inbound_nodes[0].state\n",
    "        e_x = np.exp(x)\n",
    "        self.state = e_x / (1+e_x)\n",
    "\n",
    "    def backward(self):\n",
    "        self.incoming_error = np.zeros_like(self.state)\n",
    "        for node in self.outbound_nodes:\n",
    "            self.incoming_error += node.outgoing_errors[self]\n",
    "        self.outgoing_errors[self.inbound_nodes[0]] = self.state * (1-self.state) * self.incoming_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented `bacward` methods for all the operations needed by most ANNs.\n",
    "\n",
    "The only things to complete in order to train an ANN are:\n",
    "* a method to generate the error signals for all the parameters and\n",
    "* a way to transform numerical error signals in actual changes to the model parameters.\n",
    "\n",
    "The first thing is accomplished by defining a `backward` method for the `MSE` node class, that should provide the agent the initial *desire*:\n",
    "\n",
    "$$\\delta y = \\frac{\\partial l}{\\partial y} \\bigg \\rvert_{\\tilde{y}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y_hat, y):\n",
    "        Node.__init__(self, inbound_nodes=[y_hat, y])\n",
    "\n",
    "    def forward(self):\n",
    "        y_hat = self.inbound_nodes[0].state\n",
    "        y = self.inbound_nodes[1].state\n",
    "        diff = y_hat - y\n",
    "        self.state = np.sum(np.square(diff)) / 2.0\n",
    "\n",
    "    def backward(self):\n",
    "        y_hat = self.inbound_nodes[0].state\n",
    "        y = self.inbound_nodes[1].state\n",
    "        diff = y_hat - y\n",
    "        self.outgoing_errors[self.inbound_nodes[1]] = -diff\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute error signals for the parameters, we need a function that takes a computational graph as input and propagates the error signals back through it, much like the function `forward_prop` defined above was used for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_prop(requirements_chain):\n",
    "    \"\"\"Propagate error signals backward through the model (but do not apply\n",
    "    corrections).\n",
    "\n",
    "    Args:\n",
    "        requirements_chain (:obj:`list` of :obj:`Node`): a sequence of\n",
    "            operations that ensures computational consistency of the model.\n",
    "\n",
    "    \"\"\"\n",
    "    reverse_chain = requirements_chain[::-1]\n",
    "    for node in reverse_chain:\n",
    "        node.backward()\n",
    "\n",
    "        \n",
    "def get_parameters_nodes(input_nodes):\n",
    "    \"\"\"Find operations containing the parameters of the model.\n",
    "\n",
    "     Args:\n",
    "        input_nodes (:obj:`list` of :obj:`Node`): the input operations of the\n",
    "            model.\n",
    "\n",
    "     Returns:\n",
    "        parameters (:obj:`list` of :obj:`Node`): the operations containing\n",
    "            the parameters of the model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = list()\n",
    "    for node in input_nodes:\n",
    "        if node.is_trainable:\n",
    "            parameters.append(node)\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the chance to add a second function `get_parameters_nodes`, in order to inspect the error signals propagated to them by a backward pass of a model.\n",
    "\n",
    "To test our implementation, we extend the `FeedforwardModelInference` class to include a `backpropagate` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss operation state:  0.0308523950479\n",
      "[[ 0.00399608  0.00405578  0.01306036 -0.00376303]\n",
      " [ 0.00396794  0.00402722  0.01296838 -0.00373653]\n",
      " [ 0.00524852  0.00532693  0.01715369 -0.00494243]]\n",
      "[[ 0.00877958  0.00891074  0.02869424 -0.00826756]]\n",
      "[[ 0.08136478]\n",
      " [ 0.18934184]\n",
      " [ 0.0517348 ]\n",
      " [ 0.16648689]]\n"
     ]
    }
   ],
   "source": [
    "class FeedforwardModelBackprop(FeedforwardModelInference):\n",
    "    def __init__(self):\n",
    "        FeedforwardModelInference.__init__(self)\n",
    "        self._build_training_ops()\n",
    "        self.graph = self.get_topological_order()\n",
    "        self.parameters = get_parameters_nodes(self.graph)\n",
    "        \n",
    "    def _build_training_ops(self):\n",
    "        self.Y_hat = Placeholder()\n",
    "        self.loss = MSE(self.Y_hat, self.y)\n",
    "        self.input_nodes.append(self.Y_hat)\n",
    "        \n",
    "    def backpropagate(self, x, y_hat):\n",
    "        self.X.forward(value=x)\n",
    "        self.Y_hat.forward(value=y_hat)\n",
    "        forward_prop(self.graph)\n",
    "        print('Loss operation state: ', self.loss.state)\n",
    "        backward_prop(self.graph)\n",
    "        for node in self.parameters:\n",
    "            print(node.incoming_error)\n",
    "            \n",
    "            \n",
    "ff_model_backprop = FeedforwardModelBackprop()\n",
    "\n",
    "x = np.random.randn(1, 3)\n",
    "y = 0.0\n",
    "ff_model_backprop.backpropagate(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's left to do is converting error signals $\\delta \\theta$ into actual changes $\\Delta \\theta$ for the parameters.\n",
    "\n",
    "We already said that error signals are numerical gradients: they identify the direction in the parameter space $\\Theta$ that should be taken to increment the loss functional value the most in a neighborhood of the present parameter value $\\theta_{i}$.\n",
    "\n",
    "The gradient descent procedure converts then the gradient into movements with the rule:\n",
    "\n",
    "$$\\Delta \\theta_{i+1} = -\\eta \\delta \\theta_{i}$$\n",
    "\n",
    "Since this optimization is performed by an extreme approximation of the loss functional (the probability measure $\\mathbb{P}(dx)$ over $X$ is approximated with a Dirac delta on the current sample $x$), this technique is called **Stochastic Gradient Descent**, or SGD.\n",
    "\n",
    "A popular variant of SGD is SGD with momentum: the direction $\\Delta \\theta_{i+1}$ is not computed trusting only the last gradient estimate $\\delta \\theta_{i}$, which is replaced by a moving average\n",
    "\n",
    "$$\\delta \\theta_{i+1} = (1-\\beta)\\delta \\theta_{i+1} + \\beta\\delta \\theta_{i}$$\n",
    "\n",
    "before being transformed into an actual update\n",
    "\n",
    "$$\\Delta \\theta_{i+1} = -\\eta \\delta \\theta_{i+1}$$\n",
    "\n",
    "The exponential moving average parameter $\\beta \\in [0, 1)$ influences the influence of the past over the current gradient: $\\beta \\approx 1$ greatly trusts the past over the present, while $\\beta \\approx 0$ relies almost only on the present sample.\n",
    "\n",
    "This consideration brings along the concept of **model trainer**, an object that supervises the ANN during training collecting the `incoming_error` signals for all `Variable` objects, keeping moving averages of updates, computing updates and finally updating them.\n",
    "This desing choice is implemented by TensorFlow's `Optimizer` classes, which we will simplify in `Trainer` classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\"Create a trainer to monitor and update models :obj:`Variable` nodes.\n",
    "\n",
    "    This object is linked to all the :obj:`Variable` nodes of a given graph, and\n",
    "    is in charge of the bookkeeping of the gradients and the application of the\n",
    "    training numerical optimization algorithm.\n",
    "\n",
    "    Attributes:\n",
    "        gradients (:obj:`dict` of :obj:`ndarray`s): the gradients of each of the\n",
    "            model's parameters, computed during last backpropagation pass.\n",
    "        learning_rate (:obj:`float`): the step length of the numerical\n",
    "            optimization algorithm.\n",
    "\n",
    "    Methods:\n",
    "        _reset_gradients: reset gradients accumulators to zero.\n",
    "        update_gradients: accumulate each parameter's `incoming_error` into the\n",
    "            corresponding gradient accumulator.\n",
    "        apply_gradients: modify each parameter's `state` according to the\n",
    "            corresponding gradient and optimization algorithm.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        \"\"\"Create the trainer and link it to models variables.\"\"\"\n",
    "        self.gradients = dict()\n",
    "        for node in parameters:\n",
    "            self.gradients[node] = np.zeros_like(node.state)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _reset_gradients(self):\n",
    "        for (node, grad) in self.gradients.items():\n",
    "            grad *= 0.0\n",
    "\n",
    "    def update_gradients(self):\n",
    "        for (node, grad) in self.gradients.items():\n",
    "            grad += node.incoming_error\n",
    "\n",
    "    def apply_gradients(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Trainer):\n",
    "    \"\"\"Vanilla Stochastic Gradient Descent (SGD) optimizer.\"\"\"\n",
    "    def __init__(self, parameters, learning_rate=0.001):\n",
    "        Trainer.__init__(self, parameters, learning_rate)\n",
    "\n",
    "    def apply_gradients(self):\n",
    "        for (node, grad) in self.gradients.items():\n",
    "            node.state -= self.learning_rate*grad\n",
    "        self._reset_gradients()\n",
    "\n",
    "\n",
    "class SGDWithMomentum(SGD):\n",
    "    \"\"\"Stochastic Gradient Descent with momentum.\n",
    "\n",
    "    Attributes:\n",
    "        beta (:obj:`float`): the coefficient of the geometric mean held between\n",
    "            past and current gradients.\n",
    "        moments (:obj:`dict` of :obj:`ndarray`s): the geometric mean of past\n",
    "            gradients.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters, learning_rate=0.001, beta=0.9):\n",
    "        SGD.__init__(self, parameters, learning_rate)\n",
    "        self.beta = beta\n",
    "        self.moments = dict()\n",
    "        for node in parameters:\n",
    "            self.moments[node] = np.zeros_like(node.state)\n",
    "\n",
    "    def apply_gradients(self):\n",
    "        for (node, grad) in self.gradients.items():\n",
    "            true_grad = (1.0-self.beta)*grad + self.beta*self.moments[node]\n",
    "            node.state -= self.learning_rate*true_grad\n",
    "            self.moments[node] = true_grad\n",
    "        self._reset_gradients()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bostonhousing></a>\n",
    "### The *Boston housing prices* problem\n",
    "\n",
    "The [*Boston housing prices* dataset](https://www.kaggle.com/c/boston-housing) is a popular toy benchmark for Machine Learning models.\n",
    "\n",
    "For many houses in Boston's suburbs, it provides numerical objective data (distance from schools and workplaces, age, pollution, crime rates, etc.) along with their cost.\n",
    "\n",
    "The problem of inferring the cost of the house giving objective data is naturally framed as a regression problem\n",
    "\n",
    "$$f: X \\to Y$$\n",
    "\n",
    "where $X = \\mathbb{R}^n$ and $Y = \\mathbb{R}$.\n",
    "\n",
    "Let's visualize the data to get a feeling for the *shape* of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIuISSsgiIi7xJzTKsjZDTamHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15a3ddefd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# load data\n",
    "file = \"./src/tests/data/boston_housing.txt\"\n",
    "dataset = pd.read_csv(file)\n",
    "X = dataset.values[:, 0:3]\n",
    "Y_hat = dataset.values[:, 3][:, None]\n",
    "\n",
    "# normalize data\n",
    "X = scale(X, axis=0)\n",
    "Y_hat = scale(Y_hat, axis=0)\n",
    "\n",
    "# plot data\n",
    "fig1 = plt.figure()\n",
    "ax = fig1.gca(projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=Y_hat, cmap='bwr')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data exhibits non-linearity and thus represents an ideal benchmark for a two-layer ANN.\n",
    "\n",
    "Using the framework developed throughout this lesson, we can elaborate on the already developed `FeedforwardModel` to build and train such an agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedforwardModel():\n",
    "    def __init__(self):\n",
    "        self.__name__ = 'BostonHousingPrices'\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 20\n",
    "        self.input_nodes = list()\n",
    "        self.inference_graph = self._build_inference_graph()\n",
    "        self.training_graph = self._build_training_graph()\n",
    "\n",
    "    def _build_inference_graph(self):\n",
    "        # input layer\n",
    "        self.X = Placeholder()\n",
    "        # hidden layer\n",
    "        num_hidden = 4\n",
    "        W1 = Variable(np.random.randn(3, num_hidden))\n",
    "        B1 = Variable(np.zeros((1, num_hidden)))\n",
    "        h = Add(Linear(self.X, W1), B1)\n",
    "        act_h = ReLU(h)\n",
    "        # output_layer\n",
    "        W2 = Variable(np.random.randn(num_hidden, 1))\n",
    "        B2 = Variable(np.zeros((1, 1)))\n",
    "        self.y = Add(Linear(act_h, W2), B2)\n",
    "        # build inference graph\n",
    "        self.input_nodes.extend([self.X, W1, B1, W2, B2])\n",
    "        graph = get_graph_flow(self.input_nodes)\n",
    "        \n",
    "        return graph\n",
    "        \n",
    "    def _build_training_graph(self):\n",
    "        self.Y_hat = Placeholder()\n",
    "        self.loss = MSE(self.Y_hat, self.y)\n",
    "        # add Trainer\n",
    "        self.input_nodes.extend([self.Y_hat])\n",
    "        parameters = get_parameters_nodes(self.input_nodes)\n",
    "        self.trainer = SGDWithMomentum(parameters, learning_rate=self.learning_rate)\n",
    "        graph = get_graph_flow(self.input_nodes)\n",
    "        \n",
    "        return graph\n",
    "        \n",
    "    def _load_data(self):\n",
    "        # load Boston housing prices data\n",
    "        file = './src/tests/data/boston_housing.txt'\n",
    "        dataset = pd.read_csv(file)\n",
    "        X = dataset.values[:, 0:3]\n",
    "        Y_hat = dataset.values[:, 3][:, None]\n",
    "        # normalize data\n",
    "        X = scale(X, axis=0)\n",
    "        Y_hat = scale(Y_hat, axis=0)\n",
    "        # create training and validation sets\n",
    "        samples = list(zip(X, Y_hat))\n",
    "        val = 0.10\n",
    "        num_val_samples = int(val * len(samples))\n",
    "        train_samples = samples[:num_val_samples]\n",
    "        valid_samples = samples[num_val_samples:]\n",
    "        \n",
    "        return train_samples, valid_samples\n",
    "\n",
    "    def train(self):\n",
    "        train_samples, valid_samples = self._load_data()\n",
    "        # training/validation statistics\n",
    "        tr_errors = list()\n",
    "        val_errors = list()\n",
    "        for i_epoch in range(self.num_epochs):\n",
    "            # training\n",
    "            train_error = 0.0\n",
    "            for sample in train_samples:\n",
    "                x, y_hat = sample\n",
    "                # forward pass\n",
    "                self.X.forward(value=x[None, :])\n",
    "                self.Y_hat.forward(value=y_hat[:, None])\n",
    "                forward_prop(self.training_graph)\n",
    "                train_error += self.loss.state\n",
    "                # backward pass\n",
    "                # compute gradients\n",
    "                backward_prop(self.training_graph)\n",
    "                self.trainer.update_gradients()\n",
    "                # apply corrections\n",
    "                self.trainer.apply_gradients()\n",
    "            tr_errors.append(train_error/len(train_samples))\n",
    "            # validation\n",
    "            valid_error = 0\n",
    "            for sample in valid_samples:\n",
    "                x, y_hat = sample\n",
    "                self.X.forward(value=x[None, :])\n",
    "                self.Y_hat.forward(value=y_hat[:, None])\n",
    "                forward_prop(self.training_graph)\n",
    "                valid_error += self.loss.state\n",
    "            val_errors.append(valid_error/len(valid_samples))\n",
    "            print(\"Epoch {:2d} - Loss: {:4.2f}\".format(i_epoch+1, val_errors[-1]))\n",
    "        plt.plot(range(self.num_epochs), tr_errors, label='Training')\n",
    "        plt.plot(range(self.num_epochs), val_errors, label='Validation')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def infer(self, x):\n",
    "        self.X.forward(value=x)\n",
    "        forward_prop(self.inference_graph)\n",
    "        \n",
    "        return self.y.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good practice when developing ANNs using Python framewors such as `deepteaching` and TensorFlow is to leverage Python's object-orientedness.\n",
    "The model we built is decomposed into graph assembly, data preparation (notice the scaling preprocessing and the partitioning of samples in 90% training samples and 10% validation samples) and training.\n",
    "Notice that during the validation phase, no backpropagation occurs, since validation is used to monitor how good is the model performance on data which has never been seen during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 - Loss: 2.06\n",
      "Epoch  2 - Loss: 1.45\n",
      "Epoch  3 - Loss: 1.09\n",
      "Epoch  4 - Loss: 0.85\n",
      "Epoch  5 - Loss: 0.70\n",
      "Epoch  6 - Loss: 0.60\n",
      "Epoch  7 - Loss: 0.54\n",
      "Epoch  8 - Loss: 0.49\n",
      "Epoch  9 - Loss: 0.46\n",
      "Epoch 10 - Loss: 0.43\n",
      "Epoch 11 - Loss: 0.41\n",
      "Epoch 12 - Loss: 0.40\n",
      "Epoch 13 - Loss: 0.39\n",
      "Epoch 14 - Loss: 0.38\n",
      "Epoch 15 - Loss: 0.37\n",
      "Epoch 16 - Loss: 0.36\n",
      "Epoch 17 - Loss: 0.36\n",
      "Epoch 18 - Loss: 0.35\n",
      "Epoch 19 - Loss: 0.35\n",
      "Epoch 20 - Loss: 0.34\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUlFGE79SSkWZ/w/c6McgKUW8DAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15a3ddef6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ff_model = FeedforwardModel()\n",
    "ff_model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "**Tensor derivatives**: this short [handouts](https://compsci697l.github.io/docs/vecDerivs.pdf) by Erik Learned-Miller are extremely clear and concise.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
