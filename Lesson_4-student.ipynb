{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4 - TensorFlow and Convolutional Neural Networks\n",
    "\n",
    "This last lesson introduces the main development framework for Artificial Neural Networks: Google's [TensorFlow](https://www.tensorflow.org) library.\n",
    "TensorFlow provides many optimization algorithms, regularization techniques, high-level abstractions (e.g. *layers*) and also logging and visualization tools collected in the [TensorBoard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) suite.\n",
    "We will take advantage of TensorFlow's API to describe the problem of regularization and the class of Convolutional Neural Networks.\n",
    "\n",
    "### Summary\n",
    "\n",
    "* [TensorFlow](#tensorflow)\n",
    "* [The MNIST dataset](#mnist)\n",
    "* [Building a model in TensorFlow](#tfmodel)\n",
    "* [Spatial locality and sparse connectivity](#sparseconnect)\n",
    "* [Layers](#layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=tensorflow></a>\n",
    "### TensorFlow\n",
    "\n",
    "TensorFlow was initially released in late 2015 to allow Machine Learning researchers to use a production-level framework for designing and testing their models.\n",
    "\n",
    "Discussions about pros and cons with respect to other frameworks are left to dedicated [blog posts](https://deeplearning4j.org/compare-dl4j-tensorflow-pytorch).\n",
    "A main point to use TensorFlow is that although it requires much effort to get confidence with, it pays off many times over due to its flexibility (passing from research models to deployment models is quite straightforward since the syntax is the same) and community support.\n",
    "\n",
    "TensorFlow is based on the *dataflow* programming model which has been studied during the last two lessons.\n",
    "This means that almost every action a programmer wants his program to do has to be thought to as an [**Operation**](https://www.tensorflow.org/api_docs/python/tf/Operation), while data is stored in [**Tensor**](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects.\n",
    "\n",
    "A tricky difference comes in when we talk about [**Variable**](https://www.tensorflow.org/api_docs/python/tf/Variable) objects: these class represents a wrapper of `tf.Operation`, `tf.Tensor` objects, and maintenance functions to allow TensorFlow to initialize, update, load and store parameters into them.\n",
    "After all, what defines an Artificial Neural Network are its architecture and its parameters: maybe TensorFlow's implementation is not the most transparent one but, as already said, it comes with many advantages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if sys.platform == 'win32':\n",
    "    WORKING_DIRECTORY = '.\\MNIST'\n",
    "elif sys.platform == 'linux':\n",
    "    WORKING_DIRECTORY = './MNIST'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=mnist></a>\n",
    "### The MNIST dataset\n",
    "\n",
    "The [MINST database](http://yann.lecun.com/exdb/mnist/) is a set of greyscale images of 28-by-28 pixels representing decimal digits from zero to nine.\n",
    "It is composed by a training set containing 60000 examples and by a test set containing 10000 examples.\n",
    "\n",
    "The problem is a multinomial logistic regression: a model $f_{\\theta}: X \\to Y$ should map images (points of $X$) to one of ten classes ($Y = \\{0, 1, \\dots 9\\}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c93884ecab60>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\matte\\Anaconda3\\envs\\deepteaching\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\matte\\Anaconda3\\envs\\deepteaching\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\matte\\Anaconda3\\envs\\deepteaching\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting .\\MNIST\\data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\matte\\Anaconda3\\envs\\deepteaching\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting .\\MNIST\\data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\matte\\Anaconda3\\envs\\deepteaching\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting .\\MNIST\\data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting .\\MNIST\\data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\matte\\Anaconda3\\envs\\deepteaching\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRysIgmAZ6WgFQRAsIx2tIAiCZf4fLjpSujvWuycAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c55356d4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(os.path.join(WORKING_DIRECTORY, 'data'), one_hot=True)\n",
    "# plot first 16 images\n",
    "batch_x, batch_y = mnist.validation.next_batch(16)\n",
    "num_rows = 4\n",
    "num_cols = 4\n",
    "fig1 = plt.figure()\n",
    "for i in range(0, num_rows*num_cols):\n",
    "    img = np.reshape(batch_x[i], (28, 28))\n",
    "    fig1.add_subplot(num_rows, num_cols, i+1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=tfmodel></a>\n",
    "### Building a model in TensorFlow\n",
    "\n",
    "TensorFlow programs should leverage Python's object-orientedness.\n",
    "\n",
    "Each `model` class should have at least a private `__build` method (called by `__init__`) and public `train` and `infer` methods:\n",
    "* the `__build` method should take care of assembling the computational graph, and may be decomposed in submethods corresponding to the mathematical transformations your model has been thinked to represent;\n",
    "* to make it easier for other researchers to replicate the model you build and its respective training experiment, training data consumed by the `train` method should be retrieved and loaded by a specific private procedure `__load_data`;\n",
    "* in contrast, it should be possible to pass whichever compatible inputs to the `infer` method, to allow quick tests on user-provided data.\n",
    "\n",
    "This approach allows other researchers and engineers to:\n",
    "* quickly reuse functions that define specific sequences of computational nodes, representing specific mathematical transformations;\n",
    "* easily reproduce your ideas;\n",
    "* quickly move from a research model to a deployment one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNISTFullyConnected():\n",
    "    def __init__(self):\n",
    "        # training hyperparameters\n",
    "        self.learning_rate = 0.005\n",
    "        self.num_epochs = 10\n",
    "        self.batch_size = 16\n",
    "        # architecture hyperparameters\n",
    "        self.num_inputs = 784\n",
    "        self.num_hidden = 40\n",
    "        self.num_outputs = 10\n",
    "        # build model and set up log tools\n",
    "        self.logdir = os.path.join(WORKING_DIRECTORY, 'log_fc')\n",
    "        self.__build(self.num_hidden)\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def __build(self, num_hidden):\n",
    "        with tf.name_scope('input_nodes') as scope:\n",
    "            self.X = tf.placeholder(tf.float32, [None, self.num_inputs], name='image')\n",
    "            ########\n",
    "            # TODO #\n",
    "            ########\n",
    "            probs = tf.nn.softmax(logits, name='probabilities')\n",
    "            self.output = tf.argmax(probs, axis=1, name='prediction')\n",
    "        with tf.name_scope('loss') as scope:\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                    labels=self.Y_hat,\n",
    "                                                                    name='cross_entropy')\n",
    "            loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "            self.loss_summary = tf.summary.scalar('loss_log', loss)\n",
    "        with tf.name_scope('optimizer') as scope:\n",
    "            self.lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "        with tf.name_scope('test') as scope:\n",
    "            correct_preds = tf.equal(self.output, tf.argmax(self.Y_hat, axis=1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32), name='accuracy')\n",
    "            \n",
    "    def __load_data(self):\n",
    "        mnist = input_data.read_data_sets(os.path.join(WORKING_DIRECTORY, 'data'), one_hot=True)\n",
    "        return mnist\n",
    "        \n",
    "    def train(self):\n",
    "        # load data\n",
    "        mnist = self.__load_data()\n",
    "        with tf.Session() as sess:\n",
    "            with tf.summary.FileWriter(self.logdir, sess.graph) as writer:\n",
    "                writer.add_graph(sess.graph)\n",
    "                # initialize variables\n",
    "                init_op = tf.global_variables_initializer()\n",
    "                sess.run(init_op)\n",
    "                num_train_batches = (len(mnist.train.images) + (self.batch_size - 1)) // self.batch_size\n",
    "                for i_epoch in range(self.num_epochs):\n",
    "                    # training\n",
    "                    for i_batch in range(num_train_batches):\n",
    "                        batch_x, batch_y_hat = mnist.train.next_batch(self.batch_size)\n",
    "                        _, loss = sess.run([self.optimizer, self.loss_summary], feed_dict={self.X: batch_x,\n",
    "                                                                                           self.Y_hat: batch_y_hat,\n",
    "                                                                                           self.lr: self.learning_rate})\n",
    "                        writer.add_summary(loss, i_epoch*num_train_batches + i_batch + 1)\n",
    "                    # validation\n",
    "                    valid_accuracy = sess.run(self.accuracy, feed_dict={self.X: mnist.validation.images,\n",
    "                                                                        self.Y_hat: mnist.validation.labels})\n",
    "                    print('Epoch: {:2d} - Validation accuracy: {:6.2f}%'.format(i_epoch+1, 100*valid_accuracy))\n",
    "                # test\n",
    "                test_accuracy = sess.run(self.accuracy, feed_dict={self.X: mnist.test.images,\n",
    "                                                                   self.Y_hat: mnist.test.labels})\n",
    "                print('Test accuracy: {:6.2f}%'.format(100*test_accuracy))\n",
    "                # save model\n",
    "                self.saver.save(sess, os.path.join(self.logdir, 'fc'))\n",
    "        \n",
    "    def infer(self, img):\n",
    "        with tf.Session() as sess:\n",
    "            ckpt = tf.train.get_checkpoint_state(self.logdir)\n",
    "            if ckpt is not None:\n",
    "                self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                lab = sess.run(self.output, feed_dict={self.X: img})\n",
    "                plt.imshow(np.reshape(img, (28, 28)), cmap='gray', label=str(lab))\n",
    "                plt.show()\n",
    "                print('Predicted label: {}'.format(str(lab)))\n",
    "            else:\n",
    "                print('No checkpoint found: train the model before doing inference.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the `deepteaching` framework, the creation and usage of an ANN model in TensorFlow must be decomposed in two steps:\n",
    "* the **build** phase, during which the computational graph is assembled;\n",
    "* the **runtime** phase, during which data is flown through the graph (and parameters stored in `Variable`s might be updated).\n",
    "\n",
    "We will discuss in detail all three main methods.\n",
    "\n",
    "#### `__build`\n",
    "The first observations regard the readability of a computational graph.\n",
    "Notice that almost each operation in TensorFlow implements a `name` attribute, that allows to give each node a descriptive label.\n",
    "More importantly, notice the usage of *scopes*.\n",
    "Scopes are **context manager** objects, i.e. Python objects that implement both a `__enter__` and an `__exit__` method.\n",
    "In particular, scopes allow a user create namespaces to group together related operations, a feature that greatly increases readability of the code (and also of the graphs that can be displayed using TensorBoard).\n",
    "\n",
    "Every input operation (`tf.placeholder`s and the `tf.Variable` objects obtained by calls to `tf.get_variable`) must declare the `shape` of their respective operands.\n",
    "This is necessary for TensorFlow to automatically set up appropriate connections between operations and test the model *dimensional consistency*.\n",
    "Observe that dimensions that are unknown at desing time, or that should remain flexible, can be set to `None`.\n",
    "This feature is very useful for placeholders, since at training time we would like to pass them batches composed by many samples (or even try different batch sizes) but at inference time we will likely process one instance at a time.\n",
    "\n",
    "Finally, we will focus on two operations which syntax does not allow an intuitive understanding of what's going on, since their functions will be completely clear only through their relations with the `train` and `infer` methods.\n",
    "\n",
    "At line 43, an operation of type `tf.summary.scalar` is defined.\n",
    "This can be thought at as a *watcher* of the underlying node (in this case the `loss` node, which computes the empirical loss whenever labelled data is fed to the model): an evaluation of a `tf.summary.scalar` produces a package of information in string format containing the last state of the underlying monitored node, and this string can be written to a `tf.Event` object.\n",
    "`tf.Event` objects can eventually be consumed by TensorBoard to visualize summaries of all monitored operations.\n",
    "\n",
    "The other important operation is defined at line 46, and is of type `tf.train.Optimizer`.\n",
    "These operations are linked on top of operations that compute losses.\n",
    "They and are much like the `Trainer` objects defined by `deepteaching`, in that they are wrappers of many smaller operations that are linked to the model.\n",
    "When a `tf.train.Optimizer` is evaluated, all its small operations are executed using two main methods:\n",
    "* `compute_gradients`, which monitors the underlying loss operation state and compute its respective error signals directed to the parameters stored in `tf.Variable`s;\n",
    "* `apply_gradients`, which implements a gradient descent algorithms using the previously computed error signals.\n",
    "\n",
    "A `minimize` method is also available, and is a wrapper that executes these two operations one after the other.\n",
    "\n",
    "#### `train`\n",
    "Pushing data through a graph in TensorFlow can happen only during `tf.Session`s.\n",
    "\n",
    "`tf.Session`s are context managers which most important method is `run`: this method is of capital importance for TensorFlow programs, since it consumes a list of `tf.Operation`s and a dictionary (called `feed_dict`) associating `tf.placeholder`s to their fed values, and returns `numpy` arrays containing the state of the passed operations after the evaluation.\n",
    "`run` is able to compute operations dynamically: it does not compute unuseful operations, and requires to feed only the placeholders which state is required to compute the desired values.\n",
    "For instance, suppose we want to run just an evaluation of the accuracy on the validation set: `loss` and `self.optimizer` are not required then, and since `self.lr` placeholder is required only by `self.optimizer` there is no need to feed this placeholder.\n",
    "\n",
    "`tf.summary.FileWriter`s are context managers which allow to monitor a `tf.Graph` passed to them at their creation.\n",
    "They can call a `add_graph` method to write a compact description of the monitored graph to a `tf.Event` (line 60).\n",
    "Notice that whenever a `tf.summary.scalar` operation is evaluated during a call to `tf.Session().run` (line 69), the output string tensor can be written to a `tf.Event` calling `tf.summary.FileWriter.add_summary` method (line 72).\n",
    "All the `tf.Event` objects are messages that can be converted to **protobuf** files (protobuf is Google's language-agnostic, platform-agnostic meta-language to describe computational entities), which can be consumed by TensorBoard for visualization purposes.\n",
    "\n",
    "Finally, `tf.train.Saver()` are context managers that can save and restore TensorFlow models (their architectures and parameters values).\n",
    "Notice that this object is added to the model after it has been built (line 14), and is called only once training has finished (line 82).\n",
    "This last choice is not a constraint of TensorFlow, which allows instead to choose criteria for checkpointing.\n",
    "Our choice to save the model only at the end of training is just to keep the code as simple as possible for didactic purposes.\n",
    "\n",
    "#### `infer`\n",
    "The first action of this method is to look for information about a trained model (`tf.train.get_checkpoint_state` method).\n",
    "If such data is found, the corresponding model is loaded into the current `tf.Session()` and the inferential part of the graph is run: an input observation `img` is fed to the placeholder `self.X`, and all operations up to `self.output` are then evaluated by the call to `run`.\n",
    "\n",
    "We can now instantiate a model and train it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-b9365ac2955e>:40: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Extracting .\\MNIST\\data\\train-images-idx3-ubyte.gz\n",
      "Extracting .\\MNIST\\data\\train-labels-idx1-ubyte.gz\n",
      "Extracting .\\MNIST\\data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\MNIST\\data\\t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  1 - Validation accuracy:  93.30%\n",
      "Epoch:  2 - Validation accuracy:  93.92%\n",
      "Epoch:  3 - Validation accuracy:  94.68%\n",
      "Epoch:  4 - Validation accuracy:  95.04%\n",
      "Epoch:  5 - Validation accuracy:  95.62%\n",
      "Epoch:  6 - Validation accuracy:  95.56%\n",
      "Epoch:  7 - Validation accuracy:  95.72%\n",
      "Epoch:  8 - Validation accuracy:  96.02%\n",
      "Epoch:  9 - Validation accuracy:  95.46%\n",
      "Epoch: 10 - Validation accuracy:  95.84%\n",
      "Test accuracy:  95.96%\n"
     ]
    }
   ],
   "source": [
    "mnist_fc = MNISTFullyConnected()\n",
    "mnist_fc.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that our TensorFlow model was properly trained, we can test it on the first instance of the batch showed above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\MNIST\\log_fc\\fc\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c514479a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: [5]\n",
      "Real label: 5\n"
     ]
    }
   ],
   "source": [
    "mnist_fc.infer(batch_x[0][None, :])\n",
    "print('Real label: {}'.format(str(np.argmax(batch_y[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard allows to inspect all the events that have been recorded by a `tf.summary.FileWriter()` object.\n",
    "In this example, we will refer to the directory containing logs as `log_dir` and to the path to the directory containing it as the `WORKING_DIRECTORY`.\n",
    "\n",
    "First, navigate to the `WORKING_DIRECTORY`:\n",
    "```\n",
    "$ cd WORKING_DIRECTORY\n",
    "```\n",
    "\n",
    "then activate TensorBoard issuing\n",
    "```\n",
    "$ tensorboard --logdir=log_dir\n",
    "```\n",
    "\n",
    "This command should print on the terminal a URL: navigating to this URL using a browser should open the TensorBoard visualization tool, that you can explore following the [official guide](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=sparseconnect></a>\n",
    "### Spatial locality and sparse connectivity\n",
    "\n",
    "The fundamental mathematical concept to understand Convolutional Neural Networks is that of **test function**.\n",
    "Formally, a test function is a map\n",
    "\n",
    "$$\\Psi: \\mathbb{R}^n \\to \\mathbb{R}$$\n",
    "\n",
    "which is **smooth**, i.e. $\\Psi \\in \\mathcal{C}^{\\infty}(\\mathbb{R}^n)$, and **compactly supported**, i.e. the set $\\mathcal{S}_{\\Psi} = \\{x \\in \\mathbb{R}^n | \\Psi(x) \\neq 0\\}$ is a compact subset of $\\mathbb{R}^n$.\n",
    "\n",
    "The key property which regards CNNs is the second, the compact support property.\n",
    "An important result of real analysis states that a subset $\\mathcal{S}_{\\Psi} \\subset \\mathbb{R}^n$ is compact if and only if it is bounded: there exists a ball of radius $\\rho \\in \\mathbb{R}^{+}$ such that $\\mathcal{S}_{\\Psi}$ is contained in such a ball, i.e. $\\|x\\| \\leq \\rho, \\forall x \\in \\mathcal{S}_{\\Psi}$.\n",
    "\n",
    "To keep things simple, imagine to close your eyes and run a hand of yours on a coarse surface.\n",
    "Clearly, if the surface is sufficiently wide you just can't feel it all under your hand (i.e. your hand is a compactly supported function) and at any fixed time step and position you receive zero information about the coarsness of the part of surface that's outside your hand; but you are *testing* the part of surface that's beneath your hand and getting **local information** about the surface!\n",
    "Hence the name of test functions.\n",
    "\n",
    "The capital importance of test functions is their regularizing property.\n",
    "In fact, just like the hand of yours of the comparison, test functions can be slided all along the tested spaces and reveal bumps, cavities, or also *averagely flat* zones: this information is extracted by multiplying the test function with the underlying region in **convolution** or **cross-correlation** operations.\n",
    "Let $h(x)$ be a function measuring a quantity, and let $\\Psi(t)$ be a test function.\n",
    "A useful feature map $\\xi(h(x))$ which extracts information from $h$ can be obtained using a convolution:\n",
    "\n",
    "$$\\begin{align} \\xi(h(x)) &= \\Psi(t) * h(x) \\\\ &= \\int_{\\mathbb{R}^n} \\Psi(t) h(x-t) dt \\end{align}$$\n",
    "\n",
    "while another feature $\\phi(h(x))$ can be obtained using the slightly different cross-correlation:\n",
    "\n",
    "$$\\begin{align} \\phi(h(x)) &= \\Psi(t) * h(x) \\\\ &= \\int_{\\mathbb{R}^n} \\Psi(t) h(x+t) dt \\end{align}$$\n",
    "\n",
    "In CNNs jargon, the operation performed by every feature map in every convolutional layer is actually a cross-correlation, although it is improperly named *convolution*.\n",
    "From now on, we will refer to cross-correlation with this improper name of *convolution*.\n",
    "\n",
    "<img src='figures/test_functions.png', width=480, height=480></img>\n",
    "\n",
    "The figure above represents a test function $\\Psi$ (dark cyan) being slided on a tested function $h$ (black).\n",
    "In the case of continuous functions, the convolution operation should be performed by placing the *center* of the test function (think of it as the top of the Gaussian bell in the figure above) on each point $x$ in the domain of $h$, and then performing the convolutional integral above.\n",
    "We observe now that the integral represents a **summation over all possible weighted products $\\Psi(t)h(x+t)dt$**, since this notion allows to understand the interpretaion of **discrete filters** employed by convolutional transformations.\n",
    "\n",
    "From a computer's perspective, no notion of a continuous function can be defined.\n",
    "The signals processed by an ANN, such as its internal features $h(x)$, are always discrete signals, almost always the result of sampling from a real *analog signal* such as a light or audio wave.\n",
    "Due to the finiteness of a computer's memory, more than discrete, each input signal $h$ is represented by a finite array, while each test function can be converted into a **discrete filter** following a similar [sampling procedure](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem).\n",
    "From now on, we will refer to these finite discrete filters simply as filters or **kernels**, meaning a discretization of an appropriate test function.\n",
    "The result of the discretization of the incoming signal $h$ and the test function $\\Psi$ is depicted in the figure below.\n",
    "\n",
    "<img src='figures/test_functions_discrete.png', width=480, height=480></img>\n",
    "\n",
    "We can see that the continuous integral is converted into a finite sum of products.\n",
    "These products multiply the non-zero components of a small vector (the filter) with the corresponding underlying elements of the discrete input signal.\n",
    "The number of non-zero components of the filter array is called the **kernel size**.\n",
    "\n",
    "A first important consequence of discretization are the constraint of the filter positions.\n",
    "We said before that in symbolic integral expressions the test function $\\Psi$ must be centered at every point $x$ in the domain of $h$ to evaluate the transform.\n",
    "Clearly, in a discrete domain this is possible only for discrete *jumps* of the filter.\n",
    "The length of these jumps is called **stride** of the convolution.\n",
    "Since both the input array and the filter array are finite, and since the filter must be convolved with elements of the input (remember that the goal of a convolution is to extract information from the underlying signal), the filter can be placed only in certain positions.\n",
    "This problem is usually solved in one of two ways:\n",
    "* with a `VALID` padding, where the left-element of the filter array is *placed* against the left-hand of the input array, and then is moved towards the right-hand of the input array by discrete jumps of size equal to `stride`, but stopping when another jump would overlap a non-zero element of the filter with an element outside the input array; supposing the input has length $i$, the kernel has size $k$ and the stride is $s$, the number of possible positions on which the filter can be placed is $o = \\lfloor \\frac{i-k}{s} \\rfloor + 1$;\n",
    "\n",
    "<img src='figures/valid_padding.png', width=480, height=480></img>\n",
    "\n",
    "* with a `SAME` padding, where we choose an element of the filter array as a *centre*, then place this element on top of the leftmost element of the input array, then move the filter rightward until the *centre* would fall upon an element which is outside the input array; clearly, this procedure might require to **pad** the input array with *fake elements* representing noise from outside the input itself; supposing again the input has length $i$, the kernel has size $k$ and the stride is $s$, the number of possible positions on which the filter can be placed is $\\o = \\lfloor \\frac{i-1}{s} \\rfloor + 1$.\n",
    "\n",
    "<img src='figures/same_padding.png', width=480, height=480></img>\n",
    "\n",
    "The natures of these two *padding schemes* answer to different situations.\n",
    "The `VALID` scheme introduces no *artificial* knowledge into the representation, but the number of positions on which the filter can be placed is no more than the number of elements in the input array; thus, this scheme leads to a **compression of the input** to a more compact feature array $\\phi$.\n",
    "The `SAME` scheme allows to place the most relevant part of a filter, its *centre*, also on the border of the input array, treating them as full-dignity array elements, where the `VALID` scheme would consider them only when they fall under the *tails* of the filter; but the price to extract information from the border is to arbitrarily add noisy context around the input array, the *padding*.\n",
    "As a sidenote: the name `SAME` comes from the fact that if we choose a filter stride of $s = 1$ for this scheme, the output array dimension $o$ is equal to the input array dimension $i$ (to make sure, just look at the equation above).\n",
    "\n",
    "A filter can be thought also as an array of the same size as the input array, but with many components set to zero.\n",
    "This property is known as **sparsity**.\n",
    "\n",
    "<img src='figures/sparse_connectivity.png', width=480, height=480></img>\n",
    "\n",
    "Convolving the same filter over different positions of an input array can thus be seen as a vector-matrix multiplication where the vector is the input array and the matrix is composed by sparse column vectors (the filter), where equally-valued elements are placed at different positions to match the corresponding underlying convolving window of the input signal.\n",
    "\n",
    "Extending this concept to two- or more-dimensional input arrays is not too difficult, since each array can be *unrolled* into a vector, and two- or more-dimensional filters can be suitably *unrolled* themselves.\n",
    "The image below depicts how an image with only one channel can be vectorized (assuming it has a [row-major memory layout](https://en.wikipedia.org/wiki/Row-_and_column-major_order)), convolved with two-dimensional filters (vectorized themselves), and then *repacked* into a two-dimensional image.\n",
    "\n",
    "<img src='figures/vectorized_convolutions.png', width=480, height=480></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=layers></a>\n",
    "### Layers\n",
    "\n",
    "TensorFlow provides a higher level, important abstraction over operations and variables.\n",
    "\n",
    "As the name says, a **layer** is an entity that is to be laid upon something else: it collects all the `Variable`s and `Operation`s needed to perform a desired high level mathematical transformation *upon* a previous feature $x$.\n",
    "For instance, it can implement a linear transformation followed by sigmoid activation\n",
    "\n",
    "$$\\phi(x) = \\sigma(xW + b)$$\n",
    "\n",
    "with a simple command\n",
    "\n",
    "```\n",
    "tf.layers.dense(x, num_units, activation=tf.nn.sigmoid, kernel_initializer=tf.truncated_normal_initializer())\n",
    "```\n",
    "\n",
    "where we suppose `x` to be an input tensor/operation.\n",
    "\n",
    "<img src='figures/dataflow_implementation_Layer.png', width=480, height=480></img>\n",
    "\n",
    "In the *dataflow* paradigm visualization we can think to layers as wrappers of simpler operations, as shown by the figure above: the first yellow layer implements an affine transformation (without activation) $s = xW + b$, the red layer implements a sigmoid activation of the score $h = \\sigma(s)$ while the last yellow layer performs a linear transformation $y = hW$ (no bias addition).\n",
    "\n",
    "The high-level view enabled by `layers` objects enormously increases modelling capabilities of researchers: it sacrifices computational efficiency (which better fits deployment requirements) for mathematical expressiveness.\n",
    "\n",
    "We will now use these classes to implement a Convolutional Neural Network to solve the MNIST multinomial regression task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-2e0518ac7acd>, line 97)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-2e0518ac7acd>\"\u001b[1;36m, line \u001b[1;32m97\u001b[0m\n\u001b[1;33m    test_accuracy = sess.run(self.accuracy, feed_dict={self.X: mnist.test.images,\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class MNISTConvolutional():\n",
    "    def __init__(self):\n",
    "        # training hyperparameters\n",
    "        self.learning_rate = 0.005\n",
    "        self.num_epochs = 10\n",
    "        self.batch_size = 16\n",
    "        # architecture hyperparameters\n",
    "        self.num_inputs = 784\n",
    "        self.num_features_conv_1 = 16\n",
    "        self.num_features_conv_2 = 32\n",
    "        self.num_features_conv_3 = 64\n",
    "        self.num_hidden = 100\n",
    "        self.keep_prob = 0.5\n",
    "        self.num_outputs = 10\n",
    "        # build model and set up log tools\n",
    "        self.logdir = os.path.join(WORKING_DIRECTORY, 'log_cnn')\n",
    "        self.__build()\n",
    "        self.saver = tf.train.Saver()\n",
    "    \n",
    "    def __build(self):\n",
    "        with tf.name_scope('input_nodes') as scope:\n",
    "            self.X = tf.placeholder(tf.float32, [None, self.num_inputs], name='raw_image')\n",
    "            x = tf.reshape(self.X, shape=[-1, 28, 28, 1], name='image')\n",
    "            self.Y_hat = tf.placeholder(tf.int32, [None, self.num_outputs], name='label')\n",
    "        with tf.variable_scope('conv') as scope:\n",
    "            conv1 = tf.layers.conv2d(inputs=x,\n",
    "                                     filters=self.num_features_conv_1,\n",
    "                                     kernel_size=[3, 3],\n",
    "                                     padding='SAME',\n",
    "                                     activation=tf.nn.relu,\n",
    "                                     name='conv1')\n",
    "            avg1 = tf.layers.average_pooling2d(inputs=conv1,\n",
    "                                               pool_size=[2, 2],\n",
    "                                               strides=[2, 2],\n",
    "                                               padding='VALID',\n",
    "                                               name='avg1')\n",
    "            conv2 = tf.layers.conv2d(inputs=avg1,\n",
    "                                     filters=self.num_features_conv_2,\n",
    "                                     kernel_size=[3, 3],\n",
    "                                     padding='SAME',\n",
    "                                     activation=tf.nn.relu,\n",
    "                                     name='conv2')\n",
    "            avg2 = tf.layers.average_pooling2d(inputs=conv2,\n",
    "                                               pool_size=[2, 2],\n",
    "                                               strides=[2, 2],\n",
    "                                               padding='VALID',\n",
    "                                               name='avg2')\n",
    "            conv3 = tf.layers.conv2d(inputs=avg2,\n",
    "                                     filters=self.num_features_conv_3,\n",
    "                                     kernel_size=[3, 3],\n",
    "                                     padding='SAME',\n",
    "                                     activation=tf.nn.relu,\n",
    "                                     name='conv3')\n",
    "            avg3 = tf.layers.average_pooling2d(inputs=conv3,\n",
    "                                               pool_size=[7, 7],\n",
    "                                               strides=[1, 1],\n",
    "                                               padding='VALID',\n",
    "                                               name='avg3')\n",
    "        with tf.variable_scope('hidden_features') as scope:\n",
    "            dense = tf.layers.dense(tf.layers.flatten(avg3), self.num_hidden, activation=tf.nn.relu, name='dense')\n",
    "            self.kp = tf.placeholder(tf.float32, name='dropout_prob')\n",
    "            dropout = tf.layers.dropout(dense, self.kp, training=True, name='dropout')\n",
    "        with tf.variable_scope('output_layer') as scope:\n",
    "            logits = tf.layers.dense(dropout, self.num_outputs, name='logits')\n",
    "            probs = tf.nn.softmax(logits, name='probabilities')\n",
    "            self.output = tf.argmax(probs, axis=1, name='prediction')\n",
    "        with tf.name_scope('loss') as scope:\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                    labels=self.Y_hat,\n",
    "                                                                    name='cross_entropy')\n",
    "            loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "            self.loss_summary = tf.summary.scalar('loss_log', loss)\n",
    "        with tf.name_scope('optimizer') as scope:\n",
    "            self.lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "        with tf.name_scope('test'):\n",
    "            correct_preds = tf.equal(self.output, tf.argmax(self.Y_hat, axis=1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32), name='accuracy')\n",
    "    \n",
    "    def __load_data(self):\n",
    "        mnist = input_data.read_data_sets(os.path.join(WORKING_DIRECTORY, 'data'), one_hot=True)\n",
    "        return mnist\n",
    "        \n",
    "    def train(self):\n",
    "        # load data\n",
    "        mnist = self.__load_data()\n",
    "        with tf.Session() as sess:\n",
    "            with tf.summary.FileWriter(self.logdir, sess.graph) as writer:\n",
    "                writer.add_graph(sess.graph)\n",
    "                # initialize variables\n",
    "                init_op = tf.global_variables_initializer()\n",
    "                sess.run(init_op)\n",
    "                num_train_batches = (len(mnist.train.images) + (self.batch_size - 1)) // self.batch_size\n",
    "                for i_epoch in range(self.num_epochs):\n",
    "                    ########\n",
    "                    # TODO #\n",
    "                    ########\n",
    "                # test\n",
    "                test_accuracy = sess.run(self.accuracy, feed_dict={self.X: mnist.test.images,\n",
    "                                                                   self.Y_hat: mnist.test.labels,\n",
    "                                                                   self.kp: 0.0})\n",
    "                print('Test accuracy: {:6.2f}%'.format(100*test_accuracy))\n",
    "                # save model\n",
    "                self.saver.save(sess, os.path.join(self.logdir, 'cnn'))\n",
    "        \n",
    "    def infer(self, img):\n",
    "        with tf.Session() as sess:\n",
    "            ckpt = tf.train.get_checkpoint_state(self.logdir)\n",
    "            if ckpt is not None:\n",
    "                self.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                lab = sess.run(self.output, feed_dict={self.X: img, self.kp: 0.0})\n",
    "                plt.imshow(np.reshape(img, (28, 28)), cmap='gray', label=str(lab))\n",
    "                plt.show()\n",
    "                print('Predicted label: {}'.format(str(lab)))\n",
    "            else:\n",
    "                print('No checkpoint found: train the model before doing inference.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We briefly describe the implementation of the current Convolutional Neural Network.\n",
    "\n",
    "Our implementation consists of three convolutions interleaved with average pooling layers, followed by two fully connected layers.\n",
    "Between these last two layers, dropout has been inserted for regularization purposes.\n",
    "The *geometry* of the model can be briefly summarized as:\n",
    "* $(28, 28, 1) \\to (28, 28, 16)$, using $[(3\\cdot3)+1]\\cdot16 = 160$ parameters;\n",
    "* $(28, 28, 16) \\to (14, 14, 16)$, using no parameters;\n",
    "* $(14, 14, 16) \\to (14, 14, 32)$, using $[(3\\cdot3\\cdot16)+1]\\cdot32 = 460$ parameters;\n",
    "* $(14, 14, 32) \\to (7, 7, 32)$, using no parameters;\n",
    "* $(7, 7, 32) \\to (7, 7, 64)$, using $[(3\\cdot3\\cdot32)+1]\\cdot64 = 18496$ parameters;\n",
    "* $(7, 7, 64) \\to (1, 1, 64)$, using no parameters;\n",
    "* $(64) \\to (100)$, using $(64+1)\\cdot100 = 6500$ parameters;\n",
    "* $(100) \\to (10)$, using $(100+1)\\cdot10 = 1010$ parameters;\n",
    "\n",
    "yielding a total of $160+460+18496+6500+1010 = 26626$ parameters.\n",
    "\n",
    "To give a better intuition of the *shape* of these transformations, we report a pictorial representation of the famous [*AlexNet* model](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), the work that triggered Deep Learning research in 2012 by unveiling the enormous statistical capabilities of Artificial Neural Networks enabled by parallel hardware.\n",
    "\n",
    "<img src='figures/alexnet.png', width=480, height=480></img>\n",
    "\n",
    "The `train` and `infer` methods are almost the same as those of the fully connected model, taking care only to feed an appropriate value to the `self.kp` placeholder that regulates the *aggressiveness* of the dropout between the last two layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\data\\train-images-idx3-ubyte.gz\n",
      "Extracting .\\MNIST\\data\\train-labels-idx1-ubyte.gz\n",
      "Extracting .\\MNIST\\data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\MNIST\\data\\t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  1 - Validation accuracy:  93.04%\n",
      "Epoch:  2 - Validation accuracy:  95.10%\n",
      "Epoch:  3 - Validation accuracy:  94.98%\n",
      "Epoch:  4 - Validation accuracy:  96.86%\n",
      "Epoch:  5 - Validation accuracy:  95.86%\n",
      "Epoch:  6 - Validation accuracy:  97.50%\n",
      "Epoch:  7 - Validation accuracy:  96.52%\n",
      "Epoch:  8 - Validation accuracy:  97.08%\n",
      "Epoch:  9 - Validation accuracy:  97.40%\n",
      "Epoch: 10 - Validation accuracy:  97.70%\n",
      "Test accuracy:  97.54%\n"
     ]
    }
   ],
   "source": [
    "mnist_cnn = MNISTConvolutional()\n",
    "mnist_cnn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training procedure reveals an accuracy comparable to that of the shallow ANN implemented above.\n",
    "\n",
    "But this *domain knowledge exploitation*, that leverages the intrinsic local structure of visual data, allowed a great saving in terms of number of parameters of the model.\n",
    "In fact, the fully connected model with forty hidden neurons used $(784+1)\\cdot40 + (40+1)\\cdot10 = 31410$ parameters, while the Convolutional one, using $26626$, is about $\\frac{31410-26626}{31410} * 100\\% \\approx 15\\%$ cheaper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .\\MNIST\\log_cnn\\cnn\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c50a4612b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: [5]\n",
      "Real label: 5\n"
     ]
    }
   ],
   "source": [
    "mnist_cnn.infer(batch_x[0][None, :])\n",
    "print('Real label: {}'.format(str(np.argmax(batch_y[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "**TensorFlow**: [Stanford's course](http://web.stanford.edu/class/cs20si/) on *TensorFlow for Deep Learning research* is accurate, complete and comes along with a nice [GitHub repository](https://github.com/chiphuyen/stanford-tensorflow-tutorials).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
