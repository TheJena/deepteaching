{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - Feedforward models\n",
    "\n",
    "This lesson will present the *dataflow* paradigm and its application to Artificial Neural Networks.\n",
    "The students will implement from scratch all the operations required to create their first Deep model, and finally use it to solve a regression task.\n",
    "\n",
    "### Summary\n",
    "\n",
    "* [Classical visualization of ANNs](#classicalANNs)\n",
    "* [The *dataflow* paradigm](#dataflow)\n",
    "* [Operations](#operations)\n",
    "* [Running a *dataflow* program](#rundataflowprog)\n",
    "* [Reinterpreting the *backpropagation* algorithm](#dataflowbackprop)\n",
    "* [Batch processing](#batch)\n",
    "* [The *Boston housing prices* problem](#bostonhousing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=classicalANNs></a>\n",
    "### Classical visualization of ANNs\n",
    "\n",
    "Our way to think to problems is strongly influenced by the way we visualize them.\n",
    "Artificial Neural Networks visualization has been strongly influenced by biological neural networks.\n",
    "The fundamental processing units, the neurons, are usually depicted as small circles plus their connections.\n",
    "In the picture below, we highlighted a neuron nucleus in gold, its synapsis (i.e. incoming signals channels) in green and its dendrites (i.e. outgoing signals channels) in turquoise.\n",
    "\n",
    "<img src='figures/neuralnet_classic.png', width=360, height=360></img>\n",
    "\n",
    "Although simple, this representation system is highly misleading from a mathematical modelling perspective.\n",
    "In fact, it induces a graph representation in our mind, thinking to neurons' nucleuses as nodes and connections as directed edges.\n",
    "It perfectly depicts the structure of the ensemble of fundamental units, but does not convey much of the **hierarchy of feature maps** that each layer of neurons applies to the preceding layers' features.\n",
    "\n",
    "We can think to the ensemble of the states of the neurons in a particular layer as coordinates of points in a vector space (or its natural extension, a **tensor space**).\n",
    "\n",
    "<img src='figures/neuralnet_classic_hierarchy.png', width=360, height=360></img>\n",
    "\n",
    "First, the ANN *looks* at a point $x \\in X$ using its input layer (the cyan one in figure); the connections between the input layer and the second (hidden) layer then transform this point into a point $\\phi(x) \\in \\Phi$ in what is called a *latent representation* or **feature** (the green one in figure); finally, the connections from the hidden layer to the output layer trasform this feature into a point $y = f(\\phi(x))$ (the magenta one in figure) in the desired output space $Y$.\n",
    "\n",
    "The main change of paradigm in visualization of ANNs is passing from node=neuron/edge=connection graphs to **node=feature map/edge=tensor data structure** graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=dataflow></a>\n",
    "### The *dataflow* paradigm\n",
    "\n",
    "[Dataflow](https://en.wikipedia.org/wiki/Dataflow_programming) is a programming model that thinks to programs as computational graphs, graphs which **nodes are operations** and which **edges contain data that are consumed and produced by the operations** they connect.\n",
    "We will call consumed data *operands* and produced data *results*.\n",
    "\n",
    "Why is the *dataflow* paradigm so important for ANNs research?\n",
    "The main reason is that graph representation of a program exposes important computational optimizations:\n",
    "* **concurrency**, due to the explicit definition of dependencies (edges);\n",
    "* **distributed computing**, thanks to the operations placement which is possible due to the operations modeling (nodes).\n",
    "\n",
    "The degree of concurrency enabled by parallel architectures and parallel programming models played a critical role in the Artificial Neural Networks boom of the last ten years.\n",
    "\n",
    "<img src='figures/google_dl.png', width=480, height=480></img>\n",
    "\n",
    "Mastering the **dataflow programming model** requires to approach the programming problem in four stages.\n",
    "The first three stages compose a **building phase** during which the graph is assembled.\n",
    "The last stage represents the **execution phase**, when the graph is *brought to life* flowing data through it.\n",
    "\n",
    "The four stages are:\n",
    "* **design** an analytical model for you problem;\n",
    "* **identify the transformations** required by the model;\n",
    "* **assemble these tranformations consistently**, i.e. respecting their hierarchical dependencies;\n",
    "* **flow data through the model**, feeding suitable input data to the first transformations and then executing other operations as soon as their required operands become available.\n",
    "\n",
    "To get a grasp on these concepts, we will go step by step through an example.\n",
    "\n",
    "During a data analysis, we **designed a statistical model**, parametric in $\\theta=(b_1, w_1, w_2)$, expressed by the analytical function:\n",
    "\n",
    "$$\\begin{align} f(x, \\theta=(b_1, w_1, w_2)) &= h(x, b_1, w_1)w_2 \\\\ &= \\sigma(\\nu(x)w_1 + b_1)w_2 \\\\ &= \\sigma((x - \\bar{x})w_1 + b_1)w_2 \\end{align}$$\n",
    "\n",
    "This decomposition highlights the dependencies of each stage of computation on preceeding elements.\n",
    "Thus we can proceed to **identify the required elementary transformations** proceeding backward from the last (higher level) operation, iteratively asking **\"Which operands are required to compute this feature?\"**:\n",
    "* the last operation is $f(x, \\theta=(b_1, w_1, w_2)) = h(x, b_1, w_1)w_2$, that it is the product between a vector $h$ and a matrix $w_2$; these operands come from distinct operations;\n",
    "* $w_2$ is a parameter of our model, so it is data that should just be emitted by some *emitting operation* which has no required dependencies; since parameters can be changed, this operation should somehow allow to emit different data depending on the moment it is executed;\n",
    "* $h = h(x, b_1, w_1)$ is itself the otput of an operation $h(x, b_1, w_1) = \\sigma(\\nu(x)w_1 + b_1)$, which requires as operand the quantity $s_1 = \\nu(x)w_1 + b_1$;\n",
    "* $s_1$ is the sum of two operands, namely $\\nu(x)w_1$ and $b_1$;\n",
    "* we can apply to $b_1$ the same reasoning that we applied to $w_2$;\n",
    "* $\\nu(x)w_1$ is the vector-matrix product of the two  operands $\\nu(x)$ and $w_1$;\n",
    "* we can apply to $w_1$ the same reasoning that we applied to $w_2$ and $b_1$;\n",
    "* $\\nu(x) = x - \\bar{x}$ is obtained as the difference betweend the independent operands $x$ and $\\bar(x)$;\n",
    "* $\\bar{x}$ is not a parameter, but an external constant that should be always emitted equal to itself (where a parameter could change);\n",
    "* finally, $x$ is the actual input to the statistical model, the observation of the reality: it should be emitted by an operation which is be fed by the real world.\n",
    "\n",
    "This analysis corresponds to the computational graph depicted below.\n",
    "\n",
    "<img src='figures/dataflow.png', width=480, height=480></img>\n",
    "\n",
    "We see that we have **input nodes** that contain:\n",
    "* data that can be fed in by the real world: **placeholder** operation;\n",
    "* constant information: **constant** operation;\n",
    "* parametric information, that can be changed as required: **variable** operations.\n",
    "\n",
    "Then, we have *real* operations in the form of **internal nodes**:\n",
    "* **sum** node;\n",
    "* **vector-matrix multiplication** nodes;\n",
    "* **sigmoid activation** node.\n",
    "\n",
    "We said before that the intermediate feature spaces represented by ANNs layers actually are vector or tensor spaces.\n",
    "Due to the inherent structure of vector and tensor spaces, points in these spaces are naturally encoded by multidimensional arrays data structures.\n",
    "Thus, **operations** (the circles representing the graph nodes) eventually consume and produce data in the form of **tensors** (the squares on the edges).\n",
    "Usually, the output tensor of the last feature map is not considered as an edge, since it is not consumed by any operation.\n",
    "\n",
    "To simplify the program management, operation placement and data transfer, operations and their output tensor are usually merged into a single implemented object called `Node`.\n",
    "\n",
    "<img src='figures/dataflow_implementation.png', width=480, height=480></img>\n",
    "\n",
    "Such an object should store basically three attributes:\n",
    "* `inbound_nodes`, a list of the **nodes whose output is required by the current node**, i.e. the nodes that provide operands to it;\n",
    "* `outbound_nodes`, a list of **nodes that will use the current node's output**, i.e. the nodes to which it provides operands;\n",
    "* `state`, an array to store **the result of the operation**; the output tensor itself!\n",
    "\n",
    "The node should also have a method:\n",
    "* `forward`, an algorithm to consume the inputs (operands) and produce its output (result).\n",
    "\n",
    "When a `Node` object is created, it should be linked to the `Node`s that contain its required operand.\n",
    "Thus, the `__init__` method of such a class should update the information about the graph's connectivity, taking a `list` of inbound `Node`s and communicating to them that the currently created `Node` requires their state (i.e. updating their `outbound_nodes` attribute).\n",
    "Notice that the connectivity information added by the constructor method of a `Node` is local, since it regards only the edged which have the current `Node` as an extremum.\n",
    "\n",
    "To visualize this, the next figure highlights a subgraph realizing the feature map $s_1 = l_1(x) = \\nu(x)w_1 + b_1$; this subgraph is composed by:\n",
    "* an arbitrary node as the current node (magenta);\n",
    "* its `inbound_nodes` (orange), containing the operands $\\nu(x)$, $w_1$ and $b_1$;\n",
    "* its `outbound_nodes` (sea green), where the output tensor $s_1$ is directed.\n",
    "\n",
    "<img src='figures/dataflow_implementation_Node.png', width=480, height=480></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, inbound_nodes=list()):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        for node in self.inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "        self.outbound_nodes = list()\n",
    "        self.state = None\n",
    "    \n",
    "    def forward(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=operations></a>\n",
    "### Operations\n",
    "\n",
    "#### Input operations\n",
    "The basic operations of a *dataflow* graph are input operations.\n",
    "These operations provide *raw data* to the program, data that is not preprocessed or transformed before being fed to the graph.\n",
    "\n",
    "This information comes in three form:\n",
    "* `Constant`, an operation which emits an operand always equal to itself; the value of the operand should be communicated to the operation at its creation;\n",
    "* `Placeholder`, an operation which emits an operand that has been read from the external environment; its `forward` method should thus use a parameter `value` which reads the current observation from the environment;\n",
    "* `Variable`, an operation which emits an operand that must be initialized by the operation's constructor method; this value is stored as the operation's state; the difference from `Constant` operations will be described in the paragraph about [backpropagation](#dataflowbackprop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Constant(Node):\n",
    "    def __init__(self, value):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def forward(self):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    \n",
    "class Placeholder(Node):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "    def forward(self, value=None):\n",
    "        # TODO\n",
    "        pass\n",
    "            \n",
    "            \n",
    "class Variable(Node):\n",
    "    def __init__(self, initial_value):\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "    def forward(self):\n",
    "        # TODO\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear maps and affine transformations\n",
    "**Linear maps** are the most important transformations that can be applied to vector and tensor spaces.\n",
    "Let $X$ and $S$ be two vector spaces, that we will call the *input space* and the *score space* respectively.\n",
    "When both these spaces are finite-dimensional, with dimensions $n$ and $m$ respectively, a linear map\n",
    "\n",
    "$$l: X \\to S$$\n",
    "\n",
    "can be algebraically described by a **vector-matrix multiplication**\n",
    "\n",
    "$$\\begin{gather} xW = \\begin{pmatrix} x_1 & x_2 & \\dots & x_n \\end{pmatrix}\n",
    "                      \\begin{pmatrix} w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "                                      w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "                                      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                                      w_{n1} & w_{n2} & \\dots & w_{nm} \\end{pmatrix}\n",
    "\\end{gather}$$\n",
    "\n",
    "The result of this operation is a vector $s \\in S$ which components are called *scores*\n",
    "\n",
    "$$s_k = \\sum_{i=1}^{n}x_i w_{ik}, k = 1, \\dots m$$\n",
    "\n",
    "The geometric meaning of this transformation is clear: the $k$-th component of $s$ is the result of a projection operation of $x$ onto the one-dimensional subspace of $X$ idetified by the $k$-th column of $W$.\n",
    "\n",
    "$$s_k = \\langle x, W^{(k)} \\rangle, k = 1, \\dots m$$\n",
    "\n",
    "Linear maps are constrained to rotations and homothecies around the origin ([polar decomposition](https://en.wikipedia.org/wiki/Polar_decomposition)), and cannot account for translations.\n",
    "To model such dependencies, a translation is usually added under the form of a **bias vector** $b \\in S$:\n",
    "\n",
    "$$\\begin{align} l: \\, &X \\to S \\\\ &x \\mapsto xW + b \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, x, w):\n",
    "        Node.__init__(self, inbound_nodes=[x, w])\n",
    "        \n",
    "    def forward(self):\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "        \n",
    "class Add(Node):\n",
    "    def __init__(self, s, b):\n",
    "        Node.__init__(self, inbound_nodes=[s, b])\n",
    "        \n",
    "    def forward(self):\n",
    "        # TODO\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear activations\n",
    "As stated by the Universal Approximation Theorem, the power of ANNs emerges when they are able to **distort space to extract non-linear features**.\n",
    "\n",
    "In order to achieve this, the output scores of linear transformations $s_k, k=1, \\dots m$ should be filtered by **sigmoid** activations\n",
    "\n",
    "$$\\begin{align} y_k &= \\sigma(s_k) \\\\ &= \\frac{e^{s_k}}{e^{s_k} + 1}\\end{align}$$\n",
    "\n",
    "In practice, many non-linear functions can play the same role.\n",
    "For example, the **hyperbolic tangent**\n",
    "\n",
    "$$\\begin{align} y_k &= \\tanh(s_k) \\\\ &= 2\\sigma(s_k) - 1 \\end{align}$$\n",
    "\n",
    "or the Leaky Rectified Linear Unit (**LeakyReLU**)\n",
    "\n",
    "$$LeakyReLU_{q}(s_k) = \\begin{cases} qs_k, & \\mbox{if } s_k \\leq 0 \\\\ s_k, & \\mbox{if } s_k > 0 \\end{cases}$$\n",
    "\n",
    "where $q \\geq 0$ is a design constant (the case $q = 0$ yields the widespreadly used **ReLU** function).\n",
    "\n",
    "Why do they exist so many activation functions?\n",
    "Should not sigmoid suffice due to the UAT?\n",
    "Although in theory the LeakyReLU and the ReLU functions do not satisfy the hypothesis of the UAT, in real instances they do, and due to their minimal computational complexity with regard to sigmoids and hyperbolic tangents are thus largely used in real models.\n",
    "Nevertheless, sigmoid and hyperbolic tangents are used since they allow more straightforward statistical interpretations:\n",
    "* since $\\sigma(s_k) \\in (0, 1)$, sigmoid units can smoothly approximate boolean behaviour (e.g. the probability that a certain feature has been detected or not);\n",
    "* since $\\tanh(s_k) \\in (-1, 1)$ carries sign information, hyperbolic tangent units can test wether a certain feature is inhibitory or excitatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    def __init__(self, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "        \n",
    "    def forward(self):\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "\n",
    "class Tanh(Node):\n",
    "    def __init__(self, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "        \n",
    "    def forward(self):\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "        \n",
    "class LeakyReLU(Node):\n",
    "    def __init__(self, q, x):\n",
    "        Node.__init__(self, inbound_nodes=[x])\n",
    "        self.q = q\n",
    "        \n",
    "    def forward(self):\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "        \n",
    "class ReLU(LeakyReLU):\n",
    "    def __init__(self, x):\n",
    "        LeakyReLU.__init__(self, 0, x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=rundataflowprog></a>\n",
    "### Running a *dataflow* program\n",
    "\n",
    "Up to now, we have developed all the components required to create a computational graph.\n",
    "But before we can run it, we have to accomplish the third stage, **assemble these operations consistently**.\n",
    "We can tackle this stage dividing it into two tasks:\n",
    "* assemble a graph;\n",
    "* automatically detect the computational dependencies between the operations in the graph.\n",
    "\n",
    "As for the assembly, opearations should be chained manually by the programmer.\n",
    "Since he is also the creator of the model, he knows which are the higher level features and which operands (i.e. which lower level abstractions) they require.\n",
    "\n",
    "To test our code, let's assemble the two-layer ANN model we analyzed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedforwardModelGraph():\n",
    "    def __init__(self):\n",
    "        self._build()\n",
    "        \n",
    "    def _build(self):\n",
    "        # TODO\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an assembled graph.\n",
    "But the `FeedforwardModelGraph` class still does not contain information on the order of execution of the operations.\n",
    "Consequently, it can not process automatically the calls to the `forward` methods of each node, because it might call them in an inconsistent order!\n",
    "\n",
    "To avoid calling `forward` methods by hand (which is also error prone), we can develop an algorithm to solve this problem automatically.\n",
    "\n",
    "Remember that the constructor methods of the `Node`s take care of storing local connectivity information.\n",
    "Since all `Node` objects in a graph store local connectivity information, we can use it to reconstruct the global topology of the graph and to extract dependencies.\n",
    "Suppose the computational graph is composed by nodes $O_1, O_2, \\dots O_p$.\n",
    "We would like a permutation $O_{i_1}, O_{i_2}, \\dots O_{i_p}$ such that all `inbound_nodes` of node $O_{i_{\\bar{j}}}$ have indices $i_j$ such that $j < \\bar{j}$.\n",
    "This problem is known as **topological sorting**.\n",
    "\n",
    "There are many ways to compute the topological sorting of a graph given its connectivity information.\n",
    "We use [Kahn's algorithm](https://en.wikipedia.org/wiki/Topological_sorting#Algorithms) for our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_connectivity(input_nodes):\n",
    "    \"\"\"Create a description of the connections of each node in the graph.\n",
    "\n",
    "    Recurrent connections (i.e. connections of a node with itself) are excluded.\n",
    "\n",
    "    Args:\n",
    "        input_nodes (:obj:`list` of :obj:`Node`): the input operations of\n",
    "            the model.\n",
    "\n",
    "    Returns:\n",
    "        graph (:obj:`dict` of :obj:`dict` of :obj:`set`): a description of the\n",
    "            graph's connectivity in terms of inbound-outbound nodes of each\n",
    "            node.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    graph = dict()\n",
    "    nodes = input_nodes.copy()\n",
    "    while len(nodes) != 0:\n",
    "        # select a node\n",
    "        current_node = nodes.pop(0)\n",
    "        # if no information has been collected yet, set up dict entry\n",
    "        if current_node not in graph:\n",
    "            graph[current_node] = {'inbound': set(), 'outbound': set()}\n",
    "        # scroll through current node's outbound nodes\n",
    "        for node in current_node.outbound_nodes:\n",
    "            # skip recurrent connections (for RNN cells)\n",
    "            if node == current_node:\n",
    "                continue\n",
    "            # if no information has been collected yet, set up dict entry\n",
    "            if node not in graph:\n",
    "                nodes.append(node)\n",
    "                graph[node] = {'inbound': set(), 'outbound': set()}\n",
    "            # add reciprocal connectivity information\n",
    "            graph[current_node]['outbound'].add(node)\n",
    "            graph[node]['inbound'].add(current_node)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def topological_sort(input_nodes, graph):\n",
    "    \"\"\"Get a consistent sequence of operations on the given graph.\n",
    "\n",
    "    Args:\n",
    "        input_nodes (:obj:`list` of :obj:`Node`): the input operations of\n",
    "            the model.\n",
    "        graph (:obj:`dict` of :obj:`dict` of :obj:`set`): a description of the\n",
    "            graph's connectivity.\n",
    "\n",
    "    Returns:\n",
    "        sorted_nodes (:obj:`list` of :obj:`Node`): a sequence of operations\n",
    "            that ensures computational consistency of the model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sorted_nodes = list()\n",
    "    unlocked_nodes = input_nodes.copy()\n",
    "    while len(unlocked_nodes) != 0:\n",
    "        # select an inbound-free node and add it the sorted list\n",
    "        # (it is ok for computation since all \"requirement\" nodes are available)\n",
    "        current_node = unlocked_nodes.pop(0)\n",
    "        sorted_nodes.append(current_node)\n",
    "        current_outbound = graph[current_node]['outbound']\n",
    "        if current_outbound is None:\n",
    "            # dead end reached\n",
    "            continue\n",
    "        for node in graph[current_node]['outbound']:\n",
    "            # free the outbound node from requiring current node\n",
    "            graph[node]['inbound'].remove(current_node)\n",
    "            # if the outbound node has no more requirements to be fulfilled,\n",
    "            # unlock it\n",
    "            if len(graph[node]['inbound']) == 0:\n",
    "                unlocked_nodes.append(node)\n",
    "\n",
    "    return sorted_nodes\n",
    "\n",
    "\n",
    "def get_graph_flow(input_nodes):\n",
    "    \"\"\"Build the network graph.\n",
    "\n",
    "    A wrapper function to automate model build.\n",
    "\n",
    "     Args:\n",
    "        input_nodes (:obj:`list` of :obj:`Node`): the input operations of\n",
    "            the graph.\n",
    "\n",
    "     Returns:\n",
    "        requirements_chain (:obj:`list` of :obj:`Node`): a sequence of\n",
    "            operations that ensures computational consistency of the model.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "    connectivity = get_connectivity(input_nodes)\n",
    "    requirements_chain = topological_sort(input_nodes, connectivity)\n",
    "\n",
    "    return requirements_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief explanation of the functions defined in the previous cell:\n",
    "* `get_connectivity` takes a list of input nodes and returns a `dictionary` storing an entry for each `Node` summarizing the connectivity of that `Node`;\n",
    "* `topological_sort` consumes this `dictionary` to produce a `list` of `Node`s starting from the list of input nodes and removing them from their *requirements role for following nodes* (line `69` of previous cell); once a `Node` is freed from all its requirements, it is marked as *unlocked* and add to the list of computable nodes;\n",
    "* `get_graph_flow` is simply a wrapper for the previous two functions.\n",
    "\n",
    "If we extend the `FeedforwardModelGraph` class with a method `_get_topological_order`, we can verify that the algorithm works as desired on this computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardModelSorted(FeedforwardModelGraph):\n",
    "    def __init__(self):\n",
    "        FeedforwardModelGraph.__init__(self)\n",
    "        self.graph = self._get_topological_order()\n",
    "        \n",
    "    def _get_topological_order(self):\n",
    "        # TODO\n",
    "        pass\n",
    "        \n",
    "        \n",
    "ff_model_sorted = FeedforwardModelSorted()\n",
    "\n",
    "for node in ff_model_sorted.graph:\n",
    "    print(node)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally **bring the graph to life by flowing data through it**.\n",
    "\n",
    "We first load the observed data into the `Placeholder` operation of the graph, then execute a *systolic* sequence of calls to the `forward` methods to all the `Node`s until the final `Node`'s `state` is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_prop(requirements_chain):\n",
    "    \"\"\"Push the current inputs through the whole model.\n",
    "\n",
    "    Consistently complete the systolic sequence of operations.\n",
    "\n",
    "    Args:\n",
    "        requirements_chain (:obj:`list` of :obj:`Node`): a sequence of\n",
    "            operations that ensures computational consistency of the model.\n",
    "\n",
    "    \"\"\"\n",
    "    for node in requirements_chain:\n",
    "        node.forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:  [[-0.4742902  -0.24960602 -1.12739249]]\n"
     ]
    }
   ],
   "source": [
    "class FeedforwardModelInference(FeedforwardModelSorted):\n",
    "    def __init__(self):\n",
    "        FeedforwardModelSorted.__init__(self)\n",
    "        \n",
    "    def infer(self, x):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "      \n",
    "ff_model_inference = FeedforwardModelInference()\n",
    "\n",
    "x = np.random.randn(1, 3)\n",
    "ff_model_inference.infer(x)\n",
    "\n",
    "print('Observation: ', x)\n",
    "print('Prediction: ', ff_model_inference.y.state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=dataflowbackprop></a>\n",
    "### Reinterpreting the *backpropagation* algorithm\n",
    "\n",
    "In the first lesson, we reformulated the learning problem in terms of dynamical systems.\n",
    "In fact, given a parametric model\n",
    "\n",
    "$$\\mathcal{F}_{\\Theta} = \\{f_{\\theta}: X \\to Y\\}_{\\theta \\in \\Theta}$$\n",
    "\n",
    "and a loss functional\n",
    "\n",
    "$$L: \\mathcal{F}_{\\Theta} \\to \\mathbb{R}$$\n",
    "\n",
    "we can use $L$ as a *torchlight* to evaluate a point $f_{\\theta_{i}} \\in \\mathcal{F}_{\\Theta}$ and take a step to get to a (hopefully) better point $f_{\\theta_{i+1}}$.\n",
    "This procedure can be iterated to yield a discrete sequence of approximations\n",
    "\n",
    "$$f_{\\theta_{0}}, f_{\\theta_{1}}, f_{\\theta_{2}}, \\dots f_{\\theta_{i}} \\dots$$\n",
    "\n",
    "What is still missing is how to use $L$ to get the sequence $\\theta_{0}, \\theta_{1}, \\theta_{2}, \\dots \\theta_{i} \\dots$ or, equivalently, the sequence of updates\n",
    "\n",
    "$$\\begin{align} \\Delta \\theta_{1} &= \\theta_{1}-\\theta_{0} \\\\\n",
    "                \\Delta \\theta_{2} &= \\theta_{2}-\\theta_{1} \\\\\n",
    "                \\dots \\\\\n",
    "                \\Delta \\theta_{i} &= \\theta_{i}-\\theta_{i-1} \\\\\n",
    "                \\dots\n",
    "\\end{align}$$\n",
    "\n",
    "When the parametric model is implemented as an ANN, the *backpropagation* algorithm generates the sequence $\\{\\Delta \\theta_{i}\\}_{i \\in \\mathbb{N}}$ in a two-steps cycle:\n",
    "* first, an approximation of the loss functional called the **empirical loss** is used to evaluate the performance of the current approximation $f_{\\theta_{i-1}}$;\n",
    "* second, the gradient descent algorithm (or more sophisticated variants of its) is used to compute an approximation of $\\nabla_{\\theta}L$ and improve the performance of the model taking a step in the direction $\\Delta \\theta_{i}$ that should minimize the loss.\n",
    "\n",
    "In statistical learning theory, the loss functional evaluated on a model $\\mathcal{F}_{\\Theta}$ is usually defined as a probability-weighted mean of the error that an approximation $f_{\\theta_{i}}$ makes when it is used to approximate the real (and unknown) map $f(x) = \\hat{y}$:\n",
    "\n",
    "$$L(f_{\\theta}) = \\int_{X} l(\\hat{y}, f_{\\theta}(x)) \\mathbb{P}(dx)$$\n",
    "\n",
    "Here, $l: Y \\times Y \\to \\mathbb{R}_0^+$ is a non-negative distance function defined on the output space $Y$, that should evaluate to zero whenever the prediction is correct ($f_{\\theta}(x) = \\hat{y}$) and to positive numbers otherwise: this function is called **loss function**.\n",
    "\n",
    "Clearly, not all points $x \\in X$ are known at training time; moreover, some of them might never be seen by the ANNs also during inference!\n",
    "Thus, the formula above is practically untractable.\n",
    "To solve this issue, the concept of *empirical loss* comes into play.\n",
    "In fact, an extreme approximation of the analytical weighted mean is\n",
    "\n",
    "$$\\begin{align} L(f_{\\theta}) &\\approx \\tilde{L}(f_{\\theta}) \\\\\n",
    "                             &= l(\\hat{y}, f_{\\theta}(x))\n",
    "\\end{align}$$\n",
    "\n",
    "i.e. the evaluation of $f_{\\theta}$ on just a single data point!\n",
    "Although highly imprecise, this computation is certainly practical.\n",
    "\n",
    "In the previous section, we built a set of components that allow us to write parametric models $f_{\\theta}$ and also to evaluate them on observable variables $x \\in X$.\n",
    "It is thus straightforward to extend the *dataflow* model to create **loss operations**.\n",
    "Loss operations should consume the output of the last inference operation and a placeholder operation emitting the correct label $\\hat{y}$, and should produce a real non-negative number representing the value of the loss function on this pair.\n",
    "\n",
    "For example, for regression tasks we could implement the Mean Squared Error (MSE) loss function:\n",
    "\n",
    "$$l(\\hat{y}, f_{\\theta}(x)) = \\frac{1}{2}(\\hat{y} - f_{\\theta}(x))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y_hat, y):\n",
    "        Node.__init__(self, inbound_nodes=[y_hat, y])\n",
    "        \n",
    "    def forward(self):\n",
    "        # TODO\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the loss is not a necessary step for parametric learning.\n",
    "It is nevertheless a fundamental step for the *backpropagation* algorithm, which goal is exactly to generate the updates to reduce the loss of the model.\n",
    "\n",
    "As stated in the previous lesson, the term *backpropagation* refers to communicating error signals back through an ANN.\n",
    "But what is exactly an error signal?\n",
    "And how is the loss function useful to this goal?\n",
    "\n",
    "Basic calculus introduces the concept of derivative of a function\n",
    "\n",
    "$$f'(x) = \\frac{df}{dx}$$\n",
    "\n",
    "to express which is the expected variation $df$ of function $f$ evaluated at $x$ if we take a positive and arbitrarily small step $dx$ moving towards $+\\infty$.\n",
    "If $f'(x) > 0$, taking such a step $dx$ means that the dependent variable $y = f(x)$ will increase; conversely, if $f'(x) < 0$ and we take such a step towards $+\\infty$, then $y = f(x)$ will decrease; finally, when $f'(x) = 0$ we interpret this as the fact that $f$ is *locally insensitive* to changes of $x$, i.e. the independent variable has little or no effect on the dependent one.\n",
    "\n",
    "Notice now that the derivative is composed by a *form* and by *instances*: $f'(\\cdot)$ is just a *formal way to process* any point $x$ belonging to $f'$'s domain, and takes numerical meanining only when it is *instantiated*, i.e. computed at a specific point $x$.\n",
    "We call the numerical value of the derivative of a function $f$ (or its high-dimensional counterpart, the gradient) computed at a specific point $\\tilde{x}$ the **error signal** directed from $l$ to $x$:\n",
    "\n",
    "$$\\nabla_{x}f \\rvert_{\\tilde{x}} = \\left(\\frac{\\partial f}{\\partial x_i} \\bigg\\rvert_{\\tilde{x_{i}}} \\right)_{i=1, 2, \\dots n}$$\n",
    "\n",
    "The geometric interpretation of the error signal is thus the same of the gradient.\n",
    "Suppose we are in a point $x \\in X$ and we are constrained to take a step, only one step, of fixed length $\\|\\Delta x\\|$.\n",
    "We would like to take this step $\\Delta x$ such that $f(x + \\Delta x) - f(x)$ is maximum.\n",
    "The direction $\\nabla_{x}f$ is the one with this desired property, while its opposed direction $-\\nabla_{x}f$ yields a variation $f(x + \\Delta x) - f(x)$ which is minimum.\n",
    "But whenever $f$ is a non-linear function, the gradient hold this desirable property only in a local neighborhood of $x$: in fact, for non-linear functions, in general $\\nabla_{x}f \\rvert_{\\tilde{x}_1} \\neq \\nabla_{x}f \\rvert_{\\tilde{x}_2}$ when $\\tilde{x}_1 \\neq \\tilde{x}_2$.\n",
    "The family of Gradient Descent algorithms from numerical optimization is particularly concerned with the search of good strategies to choose steplength parameters $\\{\\alpha_{i}\\}_{i \\in \\mathbb{N}}$ such that in practice the local descent directions $\\{\\nabla_{x}f \\rvert_{\\tilde{x}_{i}}\\}_{i \\in \\mathbb{N}}$ are only followed in a neighborhood where the descent property holds.\n",
    "\n",
    "In the parametric statistical learning, these calculus and numerical optimization methods can be used treating the parameters $\\theta \\in \\Theta$ as variables and the loss function $l$ as the objective function to be minimized.\n",
    "The direction of steepest ascent can in fact be computed as:\n",
    "\n",
    "$$\\nabla_{\\theta}l \\rvert_{\\tilde{\\theta}} = \\left(\\frac{\\partial L}{\\partial \\theta_{i}} \\bigg \\rvert_{\\tilde{\\theta_{i}}}\\right)_{i = 1, 2, \\dots n}$$\n",
    "\n",
    "and the movement in the *parameter state space* (remember that we interpret the learning procedure as a discrete time dynamical system) can be computed as:\n",
    "\n",
    "$$\\Delta \\theta = -\\alpha \\nabla_{\\theta}l \\rvert_{\\tilde{\\theta}}$$\n",
    "\n",
    "In Deep Learning research, the steplength $\\alpha$ is called *learning rate* and is sometimes denoted with $\\eta$.\n",
    "\n",
    "Two points are still missing from our analysis of *backpropagation*:\n",
    "* how to actually compute errors through a two-or-more layers model;\n",
    "* how to implement the procedure in the *dataflow* paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=batch></a>\n",
    "### Batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=bostonhousing></a>\n",
    "### The *Boston housing prices* problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
